WEBVTT

00:00.000 --> 00:22.260
Луднева, который у нас находится в дата-практике, один из тоже евангелистов, тех ребят, которые активно двигают тему CE, в нее верят, но и бэкграунд мешает инженерия, то есть глубокое понимание того, как в принципе модели устроены, как модели работают.

00:22.260 --> 00:34.760
И сегодня у нас такая будет история техническая в какой-то части, но постараемся ее тоже достаточно простым языком, с примерами, с кодом показать.

00:35.120 --> 00:42.100
Миш, тогда, наверное, тебе дальше слово. Перехватывай микрофон и вперед.

00:43.940 --> 00:46.560
Спасибо. Добрый день, коллеги.

00:46.560 --> 01:01.180
Сегодня у нас занятие в воркшоп. Цель создать RAG-систему Retrieval Augmented Generation – это генерация, дополненная поиском.

01:01.180 --> 01:13.640
В этих слайдах много аббревиатур, и все их я старался раскрывать в нижней части слайда, для того, чтобы как-то пояснять контекст.

01:13.640 --> 01:21.900
О чем мы говорим. План на сегодня. Начнем мы с самого интересного, с десерта, как надо.

01:23.020 --> 01:31.640
Вызовем языковую модель, а потом создадим интерфейсы, пользовательский интерфейс и программный интерфейс.

01:31.640 --> 01:41.320
Затем посмотрим, какие бывают модели, как их выбирать, на каком репозитории они находятся, как они вообще выполняются.

01:41.320 --> 01:50.360
Далее мы перейдем к кодирующим и классифицирующим моделям, поскольку начнем мы с генерирующих моделей.

01:50.580 --> 01:54.840
Мы потом познакомимся с принципиально иным видом моделей.

01:54.840 --> 02:03.840
Потом будем использовать их для поиска в векторном хранилище, узнаем, что такое векторное хранилище, семантический поиск.

02:04.840 --> 02:09.100
И потом он уже станет компонентом нашей RAG-системы.

02:10.400 --> 02:11.000
Такой план.

02:12.160 --> 02:14.360
По плану есть у вас вопросы?

02:15.980 --> 02:17.120
Что-то нужно пояснить?

02:17.120 --> 02:28.500
Ну, Миш, мне кажется, стоит пару слов буквально про RAG сказать, для чего нужен с точки зрения бизнес-кейсов, чтобы ребята на входе помнимали.

02:28.620 --> 02:31.300
Дальше я думаю, что по плану это будет уже в деталях понятно.

02:32.300 --> 02:35.700
Поясни просто, что это такое, зачем это нужно.

02:36.600 --> 02:40.920
Дженни вчера упоминал, но я думаю, что здесь ты лучше раскроешь тему.

02:40.920 --> 02:53.760
Языковые модели хорошо знают язык и, можно сказать, обладают таким common sense, способны к умозаключениям, скажем так.

02:54.060 --> 02:58.760
Но ничего не знают про наш домен, про нашу предметную область, конкретную задачу.

02:59.620 --> 03:07.480
И RAG, он позволяет их быстро адаптировать к запросам из нашего окружения.

03:07.480 --> 03:11.520
Вот вкратце так, и сейчас мы посмотрим, как это реализуется.

03:16.120 --> 03:20.600
Так, это архитектура первой нашей лабораторной.

03:22.060 --> 03:25.400
Мы будем делать зеленый свиток, Python Script.

03:25.560 --> 03:31.600
Python Script будет с помощью библиотеки OpenAI Python вызывать сервис языковой модели.

03:32.000 --> 03:36.620
В качестве сервиса языковой модели мы будем использовать сервис от компании Mistral.

03:36.620 --> 03:44.280
Одна из моделей, которая у них запущена, это Mistral Small Latest.

03:44.520 --> 03:49.540
И эту модель мы вызовем для генерации текста по запросу пользователя.

03:52.200 --> 03:56.660
Вторая часть этого упражнения – это создание интерфейса.

03:57.360 --> 04:00.560
Добавление интерфейса к этой небольшой логике.

04:00.560 --> 04:08.300
К нашему скрипту мы добавим две библиотеки и предоставим пользовательский интерфейс и программный интерфейс.

04:08.900 --> 04:15.840
Это будет такой своеобразный каркас для нашего хакатона, ваших работ.

04:15.840 --> 04:21.840
Так, и открываем первую лабораторную.

04:23.440 --> 04:29.160
Это ноутбуки на сервисе Google Collab, компания Google.

04:32.660 --> 04:34.260
Так, страничка открылась.

04:34.260 --> 04:41.240
Для того, чтобы эти ноутбуки работали, мы вот здесь вот в левой части под ключиком определяем ключи.

04:42.180 --> 04:47.140
Нам нужно три секретных значения, называются секреты.

04:47.620 --> 04:53.200
Вот они у нас должны быть определены, как здесь написано в первом блоке.

04:53.200 --> 05:02.020
Да, если вы эти ноутбуки открыли, то имеет смысл запустить первую команду установки этих зависимостей,

05:03.100 --> 05:05.600
потому что она занимает некоторое время.

05:08.560 --> 05:10.080
Да, это я упустил.

05:10.220 --> 05:11.880
Мы сейчас теряем на этом время.

05:12.440 --> 05:17.840
И другой блок у меня параллельно пока не запустится.

05:19.440 --> 05:20.480
Да, он будет ожидать.

05:23.200 --> 05:35.380
Значит, какой кусок кода мы будем использовать?

05:35.380 --> 05:39.440
А, если нам больше скакать.

05:43.600 --> 05:44.620
Не знаю.

05:45.620 --> 05:46.240
Надо смотреть.

05:46.340 --> 05:53.180
Компания OpenAI известна тем, что создала сервис ChatGPT.

05:54.200 --> 05:59.040
Да, и у этого сервиса был интерфейс, и он стал де-факто стандартом.

05:59.380 --> 06:03.820
Вот, и теперь на него ссылаются как OpenAI API.

06:05.380 --> 06:10.440
Вот, и этот же API стал использоваться многими их конкурентами,

06:10.620 --> 06:13.340
и, в общем, стал таким де-факто стандартом индустрии.

06:13.340 --> 06:25.340
Поэтому появилась библиотека OpenAI Python, которая тоже используется как каркас для создания многих библиотек,

06:25.440 --> 06:31.460
аналогичных библиотек компаний-конкурентов, которые предоставляют альтернативные решения.

06:31.460 --> 06:37.180
То есть, скорее всего, если вы будете вызывать какую-то языковую модель,

06:37.420 --> 06:41.700
вы, скорее всего, будете ее делать с помощью библиотеки OpenAI Python.

06:42.700 --> 06:46.420
Вот здесь, в первой строчке этого блока, она импортируется.

06:47.420 --> 06:51.400
Да, я сказал, что она широко распространена, но надо упомянуть исключения.

06:51.400 --> 06:59.860
Скорее всего, Яндекс и Гигачат, да, они идут своим путем.

07:00.600 --> 07:04.240
У них нет такой интерфейс, и вот если я ничего не путаю,

07:04.560 --> 07:07.780
то вызываются они через другие библиотеки.

07:08.640 --> 07:11.860
Вот, но сегодня мы работаем с сервисом компании Мистраль,

07:11.960 --> 07:17.880
которая вполне себе вызывается через эту общую библиотеку OpenAI,

07:17.880 --> 07:22.260
хотя у них в документации вы увидите, что они рекомендуют свою библиотеку использовать.

07:23.680 --> 07:27.100
Возможно, их библиотека также основана на OpenAI,

07:27.240 --> 07:30.220
но вот это я не проверял, но так часто бывает.

07:30.920 --> 07:33.520
Значит, единственное усложнение в этом скрипте,

07:33.620 --> 07:37.860
вот я использую вот эти функции для получения секретных значений,

07:38.080 --> 07:41.880
для того, чтобы, поделившись ноутбуком,

07:42.660 --> 07:45.040
не поделиться этими секретными значениями.

07:45.380 --> 07:46.880
Чуть-чуть замысловато.

07:46.880 --> 07:48.740
Немного усложняет код.

07:49.460 --> 07:52.680
В общем, для подключения к сервису языковой модели

07:52.680 --> 07:54.700
надо передать секретный API-ключ,

07:54.960 --> 07:57.520
который вы загенерировали предварительно,

07:57.760 --> 07:59.260
и базовый URL.

07:59.360 --> 08:02.220
Базовый URL, потому что, значит, библиотека предназначена,

08:02.400 --> 08:05.380
чтобы вызывать сервис компании Сэма Альтмана,

08:05.600 --> 08:08.760
но теперь мы можем вызывать любые сервисы,

08:08.880 --> 08:10.700
и поэтому указываем их адрес.

08:10.700 --> 08:17.940
Ну и когда вы у себя будете запускать эти языковые модели,

08:18.060 --> 08:21.040
вы просто будете указывать другой базовый URL.

08:24.620 --> 08:28.900
Через вызов этой функции происходит вызов языковой модели.

08:28.900 --> 08:32.480
Здесь мы используем интерфейс чат-комплишен.

08:33.840 --> 08:37.340
Упомяну, что есть более простой интерфейс вызова.

08:37.540 --> 08:40.240
Есть вызов без слова чат.

08:40.980 --> 08:42.760
Есть просто вызов комплишенов.

08:42.760 --> 08:47.040
Вот, но мы как бы не будем его использовать.

08:47.680 --> 08:49.340
Это отдельная тема.

08:49.400 --> 08:50.360
В общем, он не так важен.

08:50.440 --> 08:52.840
Давайте сконцентрируемся над чат-комплишеном.

08:53.320 --> 08:54.800
Чат-комплишен, и вот это важно,

08:54.940 --> 08:57.180
получает массив сообщений.

08:57.780 --> 09:01.680
Каждое сообщение – это словарь, да,

09:01.820 --> 09:06.000
тапл из двух значений – роль и контент.

09:07.000 --> 09:09.840
Вот, и вот эти значения роли,

09:09.840 --> 09:13.260
они очень важны, потому что взаимодействие с языковой моделью,

09:13.280 --> 09:16.360
оно строится через сообщение определенных ролей.

09:17.960 --> 09:21.280
И дальше массив сообщений тоже важен,

09:21.380 --> 09:25.100
потому что вот диалог с языковой моделью ведется

09:25.100 --> 09:28.440
созданием вот этого списка сообщений,

09:28.440 --> 09:33.120
потому что модель, она, контекст диалога не хранит,

09:33.200 --> 09:37.440
она стейтлесс, поэтому мы его в каждом запросе пересылаем,

09:37.440 --> 09:40.800
пересылаем, добавляя сообщения в этот массив.

09:41.440 --> 09:43.640
Я надеюсь, дальше будет более понятно.

09:44.300 --> 09:46.220
Также передаем название модели,

09:46.300 --> 09:50.100
потому что модель можно использовать другую,

09:50.460 --> 09:52.700
любую другую, они все по-разному стоят.

09:53.460 --> 09:54.820
Вот, и если нам повезет,

09:55.820 --> 10:00.180
мы вызовем языковую модель,

10:00.240 --> 10:02.660
и она вернет нам результат.

10:02.840 --> 10:05.340
Да, она нам рассказала про китайскую комнату.

10:05.340 --> 10:10.920
Здесь вот этим вот куском кода есть какие-то вопросы,

10:11.100 --> 10:12.660
надо какие-то еще пояснения дать,

10:12.820 --> 10:14.260
там понятно, зачем мы это делали.

10:16.840 --> 10:18.780
А если мы передаем много сообщений,

10:18.860 --> 10:20.120
это как-то ограничивает наш контекст,

10:20.240 --> 10:21.280
который мы можем передавать?

10:21.680 --> 10:25.020
То есть это жирает контекст или нет?

10:25.400 --> 10:28.120
Да, все сообщения должны поместиться в контекст.

10:28.760 --> 10:30.200
И с этим ничего нельзя поделать.

10:30.200 --> 10:32.880
Это вот базовое ограничение.

10:33.160 --> 10:39.580
Оно объясняется тем, что вот мы в чат-гпт-интерфейсе

10:39.580 --> 10:40.460
ведем диалог, да?

10:40.940 --> 10:43.100
Каждый раз мы как бы одно сообщение отсылаем,

10:43.160 --> 10:44.260
и у нас появляется иллюзия,

10:44.420 --> 10:47.680
что на той стороне нас кто-то понимает и помнит, да?

10:47.800 --> 10:49.040
И с нами ведет диалог.

10:49.160 --> 10:51.720
Но по факту вот внутри это работает именно так.

10:52.720 --> 10:55.200
Вот эти вот цепочки сообщений каждый раз

10:55.200 --> 10:58.540
отправляется туда, и модель ее перечитывает.

10:59.900 --> 11:02.620
Надо упомянуть, что там много всяких оптимизаций.

11:03.100 --> 11:04.820
И еще одно исключение из этого.

11:05.580 --> 11:09.020
Надо упомянуть, что есть такое mem-GPT.

11:09.300 --> 11:13.180
Это когда появляется сообщение memory updated.

11:14.360 --> 11:17.000
Но это тоже сейчас как бы неважно.

11:17.100 --> 11:18.400
Вот сейчас надо сконцентрироваться

11:18.400 --> 11:22.120
над этой общей, ну, основной моделью взаимодействия

11:22.120 --> 11:23.140
с языковыми моделями.

11:23.140 --> 11:26.540
Дальше у нас появляется интерфейс.

11:26.640 --> 11:28.240
Я имею в виду маленький комментарий.

11:28.300 --> 11:30.220
Не знаю, ребята обратили внимание или нет.

11:30.600 --> 11:33.940
Можно у себя открыть тоже параллель ноутбук

11:33.940 --> 11:35.460
и пробовать руками.

11:35.540 --> 11:37.940
Если ключик вам удалось сгенерить в мистерале

11:37.940 --> 11:41.040
по инструкции, то вот это все должно работать

11:41.040 --> 11:43.380
у вас в том числе локально на ваш ноутбук.

11:45.380 --> 11:46.160
Да, спасибо.

11:46.160 --> 11:46.220
Спасибо.

11:46.220 --> 11:55.800
Ну, или там после воркшопа у вас, если будут вопросы,

11:57.240 --> 11:59.520
то есть давайте эти ворбуки использовать,

11:59.700 --> 12:01.560
они должны у вас заработать для того,

12:01.620 --> 12:05.380
чтобы дать вам возможность экспериментировать с этими системами.

12:05.380 --> 12:10.580
Дальше в две строчки создаем пользовательский интерфейс.

12:11.480 --> 12:14.720
Для создания пользовательского интерфейса

12:14.720 --> 12:16.380
мы используем библиотеку Gradio,

12:17.660 --> 12:21.920
тоже очень распространенная библиотека в этой сфере.

12:22.780 --> 12:25.660
Она позволяет с помощью Python-классов

12:25.660 --> 12:29.480
рисовать довольно приятные функциональные интерфейсы.

12:29.480 --> 12:35.420
Ну, и сразу предоставляет пользовательский фронт-энд

12:35.420 --> 12:38.040
для этих приложений.

12:39.960 --> 12:43.000
Собственно, все создание интерфейса,

12:43.060 --> 12:46.820
оно заключается вот в этих двух командах.

12:46.940 --> 12:48.860
Здесь мы создаем чат-интерфейс,

12:48.860 --> 12:52.800
и в нем уже сразу идет диалог с языковой моделью.

12:53.760 --> 12:55.600
Так вот, оно уже появилось.

12:55.600 --> 12:59.720
И методом launch мы запускаем это приложение.

13:00.940 --> 13:02.720
Что из интересного здесь происходит?

13:02.820 --> 13:06.700
Вот мы уже получили наше приложение.

13:06.800 --> 13:08.240
Оно прямо здесь уже доступно.

13:08.300 --> 13:09.580
Это пользовательский интерфейс,

13:10.300 --> 13:12.360
с которым можно общаться.

13:20.540 --> 13:21.640
Так, все.

13:22.820 --> 13:24.460
Происходит генерация ответа.

13:25.600 --> 13:31.240
Что здесь следует упомянуть?

13:31.600 --> 13:33.500
Смотрите, мы создаем чат-интерфейс,

13:33.600 --> 13:36.480
его поведение определяется вот этим аргументом.

13:36.540 --> 13:38.960
fn, function, function onSubmit.

13:39.120 --> 13:41.340
Это мы ссылаемся на функцию,

13:41.500 --> 13:44.720
которая будет исполнена этим градео,

13:44.760 --> 13:46.740
когда мы отправим очередное сообщение.

13:46.740 --> 13:51.740
Так незамысловато начинает работать это приложение.

13:52.540 --> 13:53.980
Это функция onSubmit.

13:54.320 --> 13:58.180
Она получает сообщение от пользователя

13:58.180 --> 14:00.660
и историю, которая накапливается

14:00.660 --> 14:02.500
вот в этом пользовательском интерфейсе.

14:03.220 --> 14:08.480
И здесь мы уже формируем последовательность сообщений.

14:08.480 --> 14:11.900
Здесь я вот передаю предыдущие сообщения в историю,

14:12.000 --> 14:15.860
чтобы мы могли поддерживать диалог.

14:15.860 --> 14:25.280
Например, если мы спросим why it is green.

14:25.640 --> 14:29.160
И вот здесь вот интересно на этот глупый уточняющий вопрос.

14:29.580 --> 14:31.840
Он начнет отвечать про небо.

14:32.100 --> 14:33.820
Про небо он начнет отвечать,

14:33.920 --> 14:35.940
потому что для него it – это точно небо,

14:36.040 --> 14:39.860
потому что предыдущее сообщение было про небо.

14:39.860 --> 14:45.740
А если бы мы не передавали историю предыдущих сообщений,

14:46.280 --> 14:48.980
то языковая модель не смогла бы точно понять,

14:49.080 --> 14:49.680
что такое it,

14:49.860 --> 14:51.400
ну, сделал бы какие-нибудь предположения.

14:52.680 --> 14:54.800
Но вот это демонстрация того,

14:54.980 --> 14:57.920
как идет диалог с языковой моделью.

14:59.420 --> 14:59.940
Дальше.

15:00.420 --> 15:01.660
Метод launch.

15:02.180 --> 15:04.280
Мы передали в него параметр share true.

15:04.440 --> 15:08.100
Share true позволил нам поделиться этим приложением

15:08.100 --> 15:09.280
со всем миром.

15:09.860 --> 15:15.340
Потому что компания, которая стоит за Градио, библиотекой,

15:15.440 --> 15:22.500
она платит еще деньги за вот такой сервис,

15:22.840 --> 15:27.000
который публикует наше приложение по таким длинным ссылкам.

15:27.120 --> 15:32.500
То есть это же приложение теперь для всего мира доступно по вот этой ссылке.

15:32.920 --> 15:36.440
Там можно поставить довольно простую защиту с пользователем паролем,

15:36.440 --> 15:38.740
если вам это нужно.

15:40.180 --> 15:45.460
То есть оно здесь же в воркбуке, в выводе пишет эту ссылку.

15:45.660 --> 15:49.220
И по этой ссылке это приложение доступно для всего мира.

15:49.220 --> 15:54.960
Здесь еще есть небольшие флаги, которые чуть-чуть меняют поведение.

15:54.960 --> 15:59.080
Но это уже надо на конкретных примерах разбираться,

15:59.220 --> 16:00.640
что и зачем указывать.

16:01.220 --> 16:01.940
Дальше.

16:02.020 --> 16:04.140
Здесь мы получили пользовательский интерфейс.

16:04.280 --> 16:08.480
То есть для демонстрации вашего прототипа это более чем достаточно.

16:08.700 --> 16:10.420
У нас уже готовый чат.

16:10.580 --> 16:14.440
Можно вести диалог с нашей моделью и наполнять его функционалом,

16:14.440 --> 16:17.400
определяя логику в этом методе onSubmit.

16:18.520 --> 16:22.620
Но нам еще может потребоваться создать программный интерфейс,

16:22.800 --> 16:26.880
допустим, чтобы какой-то программной системой вызвать наше приложение.

16:27.820 --> 16:35.520
И для этого очень часто используется библиотека FastAPI.

16:35.520 --> 16:39.500
И эта библиотека FastAPI используется библиотекой Gradio.

16:39.500 --> 16:43.700
Но чтобы нам сейчас отдельно FastAPI приложение не строить,

16:43.800 --> 16:46.360
мы просто получим его из Gradio.

16:46.520 --> 16:47.900
То есть Gradio, когда стартует,

16:47.980 --> 16:50.820
оно внутри создает каркас FastAPI.

16:51.240 --> 16:54.820
И мы можем получить приложение FastAPI, которое здесь уже есть.

16:55.940 --> 16:59.960
И определить, вот так незамысловато определить,

16:59.960 --> 17:04.360
обработку запроса для API, для программного интерфейса.

17:04.360 --> 17:08.660
Здесь можно обрабатывать пост-запросы,

17:08.780 --> 17:12.420
получать сложные объекты, определяя их здесь в лаворит.

17:12.540 --> 17:17.300
То есть это такая традиционная Python-разработка программных сервисов.

17:17.420 --> 17:20.260
Например, здесь я определил метод Hello,

17:20.440 --> 17:25.480
который получает единственный аргумент Message.

17:25.480 --> 17:32.480
И мы сможем на этом же сервисе вызвать...

17:32.480 --> 17:37.780
Так, все забыл.

17:39.040 --> 17:46.480
Вызываем этот метод, передаем ему аргумент какой-то.

17:46.480 --> 17:49.580
Да, что произошло?

17:50.900 --> 17:52.740
Вот этот метод вызвался.

17:53.900 --> 17:58.480
В результате моего запроса вызвался метод OnSubmit.

17:59.000 --> 18:01.340
И снова исполнилась наша логика.

18:01.520 --> 18:04.860
И этот запрос отправился в языковую модель.

18:04.860 --> 18:08.400
То есть я поздоровался с языковой моделью, и она ответила.

18:10.960 --> 18:15.300
Вот этот простой каркас, на котором вы можете строить приложение,

18:15.300 --> 18:17.840
которое использует языковые модели.

18:19.040 --> 18:21.260
На этом у меня все.

18:21.680 --> 18:23.160
Мы возвращаемся к слайдам.

18:23.740 --> 18:24.740
Можете задать вопрос.

18:25.000 --> 18:27.500
Так, я смотрю какие-то вопросы в чате.

18:29.240 --> 18:31.720
Градио умеет ли оттачить?

18:32.040 --> 18:34.080
Да, умеет оттачить.

18:34.080 --> 18:35.220
Так, поступил вопрос.

18:35.540 --> 18:38.460
Можно ли в Градио сделать интерфейс,

18:38.560 --> 18:41.000
который позволит приоттачить документы?

18:41.700 --> 18:42.880
Да, можно.

18:44.240 --> 18:45.060
Да, можно.

18:45.300 --> 18:50.480
Это у меня упоминается на следующем слайде.

18:50.640 --> 18:53.720
Смотрите, я показал каркас с чат-интерфейсом.

18:54.020 --> 18:55.840
Одна команда делает чат.

18:56.680 --> 19:00.040
Следующий шаг расширения – это передача аргументов

19:00.040 --> 19:02.260
additional input, additional output.

19:02.260 --> 19:04.580
Можете в документации посмотреть,

19:04.840 --> 19:07.980
как мы можем добавлять еще несколько элементов

19:07.980 --> 19:10.300
в этот чатовый диалог.

19:10.300 --> 19:13.620
Если диалог становится еще сложнее,

19:13.760 --> 19:15.480
следующий этап – это мы изучаем документацию

19:15.480 --> 19:17.400
по движку blocks.

19:17.600 --> 19:21.480
На блоках мы можем построить очень более сложный интерфейс.

19:21.480 --> 19:30.200
В общем, такое постепенное усложнение сложности интерфейса,

19:30.360 --> 19:32.220
мощности этой библиотеки.

19:32.220 --> 19:36.300
Так, ссылка на документы.

19:37.560 --> 19:38.460
Файл Explorer.

19:39.280 --> 19:41.020
Да, ну, файлы прикладываются.

19:41.020 --> 19:45.400
Это реализуемые более чем достаточно, по-моему,

19:45.480 --> 19:46.940
для простых приложений.

19:47.400 --> 19:49.640
И Gradio – это не только прототипы.

19:49.760 --> 19:52.420
Вот публичные сервисы, там, чат-бот-арена,

19:53.060 --> 19:54.760
они прям созданы на Gradio.

19:54.860 --> 19:59.340
И вот эти вот два ромбика прям часто можно видеть,

19:59.480 --> 20:02.840
если вы занимаетесь языковыми моделями.

20:02.960 --> 20:06.500
То есть вот такая питоновская библиотека,

20:06.580 --> 20:08.640
она еще и для продакшен-нагрузки в интернете

20:08.640 --> 20:09.900
вполне себе используется.

20:10.940 --> 20:12.940
Альтернатива ее – это Streamlit.

20:13.140 --> 20:16.640
Из того, что я читал, аналог полный,

20:17.060 --> 20:18.900
похожим образом очень устроено,

20:19.100 --> 20:23.460
но у меня, к сожалению, не было опыта со Streamlit.

20:25.840 --> 20:29.000
Так, переходим к следующей теме.

20:29.000 --> 20:35.780
Значит, логически, да, как эти языковые модели

20:35.780 --> 20:37.480
генерируют текст.

20:38.640 --> 20:40.940
Так, поступил вопрос.

20:41.140 --> 20:42.980
Вначале говорили про лабы.

20:43.100 --> 20:45.120
Это рейсотовские лабы или на кампусе.

20:45.200 --> 20:50.120
Так, под лабами я подразумеваю вот эти три ноутбука

20:50.120 --> 20:52.060
на Google Коллабе.

20:53.980 --> 20:57.600
На них есть ссылка сверху.

20:58.740 --> 21:02.420
Да, вот в чате первая ссылка, там ссылка номер три.

21:02.420 --> 21:06.000
Это каталог с ноутбуками.

21:06.880 --> 21:08.560
Они у вас должны открываться.

21:09.320 --> 21:10.660
Вы можете их себе копировать.

21:11.760 --> 21:13.720
И это то, что я показываю сейчас.

21:15.720 --> 21:20.860
Вы копируете себе и можете продолжать ваши эксперименты,

21:20.980 --> 21:23.940
там меняете их, делаете всякие тесты.

21:23.940 --> 21:27.620
Виталий, я ответил на вопрос?

21:28.400 --> 21:29.740
Да, спасибо большое.

21:30.780 --> 21:36.080
Так, теперь посмотрим, что внутри происходит у этих моделей.

21:36.080 --> 21:40.400
До сих пор мы видели модели, которые называются GPT,

21:40.680 --> 21:43.120
генерирующий тренированный трансформер.

21:43.260 --> 21:46.800
Трансформер – это название очень удобного алгоритма,

21:46.940 --> 21:49.880
который позволил очень хорошо им обучаться.

21:50.460 --> 21:54.260
Важно, что они генерирующие, они генерируют текст,

21:55.020 --> 21:56.200
и они тренированные.

21:56.320 --> 21:59.580
Они видели много текста во время тренировки

21:59.580 --> 22:02.420
и выполняли много заданий во время тренировки.

22:02.520 --> 22:04.660
Давайте посмотрим, как происходит генерация.

22:05.440 --> 22:09.460
На вход у нас поступает одна последовательность.

22:10.480 --> 22:11.560
Здесь стоят номера.

22:11.880 --> 22:13.580
Да, последовательность номеруется.

22:13.940 --> 22:14.720
Это тоже важно.

22:14.840 --> 22:16.600
Но более важно то, что здесь не показано,

22:16.720 --> 22:18.980
что сначала отрабатывает токенайзер.

22:20.160 --> 22:24.740
Условно скажем, что все слова получают свои номера

22:24.740 --> 22:26.820
из определенного словаря.

22:26.900 --> 22:28.400
Это слоги, конечно.

22:28.400 --> 22:31.440
Вот то, что называется токенами.

22:32.020 --> 22:38.020
То есть текстовый ввод перерабатывается на последовательность чисел.

22:38.820 --> 22:41.500
И это важно, потому что токенайзер этот,

22:41.580 --> 22:45.960
он так или иначе определяет возможности модели

22:45.960 --> 22:47.260
и сильно на нее влияет.

22:48.340 --> 22:50.420
Дальше вот эта последовательность цифр,

22:50.960 --> 22:53.800
чисел поступает в языковую модель.

22:53.800 --> 22:59.340
И в результате вычисления по этой входной последовательности

22:59.340 --> 23:01.260
языковая модель генерирует слово.

23:01.380 --> 23:03.500
Но генерирует она его в следующем виде.

23:03.640 --> 23:10.020
Оно выдает колонку вероятностей следующего слова.

23:10.020 --> 23:12.560
Здесь оно в левом нижнем углу.

23:14.200 --> 23:16.100
Это показано в виде таблицы.

23:16.180 --> 23:19.160
Здесь видно, что самое вероятное слово – это capes.

23:19.700 --> 23:22.400
Другие слова менее вероятные.

23:22.500 --> 23:26.840
Ну и потом происходит этап выбора наиболее вероятного слова.

23:26.840 --> 23:31.020
И причем мы можем выбирать не только максимальную вероятность,

23:31.100 --> 23:34.560
а мы можем чуть-чуть расширить окошко выбора вероятностей,

23:35.040 --> 23:38.220
то есть взять немножко менее вероятных слов и кинуть кубик,

23:38.720 --> 23:44.000
выбрать из них одно из возможно приемлемо вероятных слов.

23:45.280 --> 23:51.280
Из этого механизма выбора происходит очень много практических последствий,

23:51.420 --> 23:53.220
такие как, например, температура.

23:53.220 --> 23:57.020
Параметр температуры как раз влияет на то,

23:57.260 --> 24:02.760
насколько будет понижена нижняя граница из возможных вероятных слов.

24:03.900 --> 24:06.320
Вот так поступил вопрос.

24:07.280 --> 24:09.660
Эта температура расширяет.

24:09.740 --> 24:12.300
Мы расширяем возможный выбор слов.

24:13.440 --> 24:17.120
Нулевая температура – это условно выбирает самое вероятное слово.

24:18.460 --> 24:20.720
И отсюда тоже можно сделать вывод,

24:20.720 --> 24:22.840
что можно генерировать параллельное.

24:22.840 --> 24:25.740
Параллельно можно сделать несколько версий текста,

24:26.240 --> 24:26.840
сгенерировать,

24:27.160 --> 24:33.840
а это повлияло на эффективность одного из этапов обучения этих моделей.

24:34.860 --> 24:35.720
Да, был какой-то вопрос?

24:38.280 --> 24:38.460
Нет?

24:39.920 --> 24:40.640
Хорошо.

24:40.960 --> 24:41.980
И что важно?

24:42.640 --> 24:45.220
У нас вот мы проделали вот эту кучу вычислений.

24:46.300 --> 24:49.580
Эта куча вычислений, она обсчитывала всю последовательность.

24:49.580 --> 24:53.380
В результате мы получили колонку вероятности и выбираем только одно слово.

24:54.480 --> 24:55.580
И вот это вот важно.

24:56.180 --> 24:57.340
Выбирается одно слово.

24:57.540 --> 25:02.440
После этого увеличенная на одно слово последовательность снова поступает на вход.

25:03.320 --> 25:04.860
Снова производится вычисление.

25:05.760 --> 25:07.740
Там, разумеется, есть какие-то оптимизации.

25:07.740 --> 25:11.640
Но в целом, в своей сути, это такой алгоритм,

25:11.780 --> 25:14.740
который для генерации одного слова просматривает всю последовательность.

25:15.580 --> 25:17.740
Причем просматривает он как бы квадратично.

25:18.460 --> 25:22.080
Каждое слово сравнивается с каждым, анализируется.

25:23.560 --> 25:26.020
Этот процесс очень вычислительно интенсивный.

25:26.140 --> 25:29.940
Для этого вот нужны там очень сложные процессоры.

25:31.260 --> 25:33.180
Генерирует он по одному слову.

25:33.180 --> 25:35.320
И что-то еще я тут хотел сказать.

25:38.700 --> 25:40.000
Нет, уже не помню.

25:43.740 --> 25:44.240
Ну ладно.

25:45.080 --> 25:46.600
Генерирует по одному слову.

25:46.820 --> 25:50.140
Есть еще некоторые алгоритмы,

25:50.300 --> 25:53.100
которые кроме слова еще получают вот эту вот вероятность.

25:53.100 --> 25:58.960
Мы в дальнейшем их упомянем.

26:01.960 --> 26:03.520
Да, а, вот еще что.

26:05.580 --> 26:08.600
Чат GPT мы когда видим, да, вот эти вот демки, интерфейсы,

26:08.700 --> 26:12.340
там слова появляются по одному такая вот анимация характерная.

26:12.420 --> 26:15.340
И возникает вопрос, ну, нельзя ли сразу все показать, да,

26:15.380 --> 26:16.100
почему так медленно.

26:16.320 --> 26:19.700
Вот оно медленно, потому что оно внутри вот так вот поэтапно,

26:19.800 --> 26:21.720
пошагово генерирует.

26:21.720 --> 26:25.000
Так оно, в общем, медленно и работает, как показывает.

26:25.200 --> 26:33.680
И в моих воркбуках, в ноутбуках, да, там вывод получается сразу за один метод, да,

26:33.740 --> 26:35.380
но если вы добавите stream true,

26:36.040 --> 26:39.940
то он начнет вот выдавать по одному токену по мере генерации.

26:40.280 --> 26:41.900
Это будет просто сложнее обрабатывать,

26:42.020 --> 26:44.700
но эта возможность, она присутствует.

26:45.660 --> 26:46.720
Так, вроде все здесь сказал.

26:46.720 --> 26:54.260
Так, следующая возможность – это структурный вывод.

26:54.360 --> 26:56.800
Структурный вывод – очень мощная штука.

26:57.360 --> 26:59.420
Смотрите, просматриваем пример.

27:00.120 --> 27:04.040
Это тоже вызов языковой модели, но с помощью программы Pro, да.

27:04.380 --> 27:06.660
Это, ну, примерно то же самое, что я сделал.

27:06.740 --> 27:09.200
Здесь вот мы передаем модель и последовательность сообщений.

27:09.200 --> 27:17.020
Здесь я, значит, создаю свой стартап, который делает шахматного тренера, да,

27:17.100 --> 27:18.480
на базе языковой модели.

27:19.320 --> 27:20.240
Что здесь происходит?

27:21.080 --> 27:23.260
Системное сообщение мы передаем.

27:23.420 --> 27:30.400
Ты тренер, получи от пользователя ход на шахматной доске, да,

27:30.480 --> 27:32.000
и ответь на этот ход.

27:32.000 --> 27:37.840
А пользователь сделал ход черной пешкой на G6.

27:38.660 --> 27:41.220
Так, чем ему отвечает система?

27:43.560 --> 27:48.120
Да, система ответит, ну, языковая модель в ответ на это

27:48.120 --> 27:52.540
ответит ему прекрасной страницей из шахматного учебника.

27:52.880 --> 27:56.160
Вот здесь все будет написано, варианты развития, там, этих партий, да,

27:56.280 --> 27:57.440
что делать, как ходить.

27:57.500 --> 28:00.320
Сразу несколько шагов ему, ходов ему подскажет.

28:00.320 --> 28:03.260
Мне, как бы, как стартаперу непонятно, куда ходить, да,

28:03.440 --> 28:08.200
как отвечать, куда двигать, да, и это вот проблема с общим подходом,

28:08.280 --> 28:10.600
с тем, что эти системы, они генерируют текст.

28:11.620 --> 28:15.660
А нам иногда надо вот в модели, да, там, выдать какой-то, грубо говоря,

28:15.720 --> 28:20.420
битовый флаг, аларм или не аларм, там, нормальный или нормальный,

28:20.500 --> 28:22.200
откуда, какую фигуру куда двинуть.

28:22.340 --> 28:24.360
Вот из этого текста очень сложно это понять.

28:24.480 --> 28:27.580
Нам нужны отдельный ряд моделей, которые примут решение,

28:28.160 --> 28:29.780
исходя из генерированного текста.

28:30.320 --> 28:33.720
Смотрите, что можно сделать со структурным выводом.

28:35.160 --> 28:40.120
Можно прям в промте, вот в этом вот, в запросе, в одном из сообщений

28:40.120 --> 28:43.340
неформально описать JSON-схему.

28:43.340 --> 28:47.800
Вот здесь, на этом слайде она описана.

28:49.940 --> 28:54.740
Она говорит, отвечай теперь в JSON-схеме.

28:55.620 --> 28:58.540
Скажи нам, какой фигурой на какую клетку поставить

28:58.540 --> 29:02.240
и оцени вероятность успешного исхода партии.

29:02.240 --> 29:09.700
И в результате теперь языковая модель ответит нам в виде JSON-а,

29:09.760 --> 29:11.140
перечислит ходы.

29:11.700 --> 29:14.400
Всегда у нас белые отвечают, всегда пешкой.

29:14.520 --> 29:18.060
На этой партии вот клетки, на которые надо поставить белую пешку,

29:18.260 --> 29:22.740
и сошлется на дебют, в который оно будет играть.

29:22.840 --> 29:23.760
Ну, дебют или защиту.

29:23.760 --> 29:26.320
И проставят вероятность исхода.

29:27.140 --> 29:30.720
И теперь, как вы понимаете, это для интеграции,

29:30.840 --> 29:33.720
для каких-то систем автоматических, которые принимают решения,

29:33.800 --> 29:36.880
это гораздо более мощный инструмент,

29:36.880 --> 29:42.080
чем генерация абзацев текста,

29:42.240 --> 29:45.560
с которыми, грубо говоря, часто непонятно, что делать.

29:47.020 --> 29:49.880
Вот эта возможность называется структурный вывод.

29:50.780 --> 29:53.680
Она требует натренированной модели на структурный вывод.

29:54.020 --> 29:57.220
В карточке модели должно быть описано, что она его поддерживает.

29:57.840 --> 30:05.360
И, кроме того, есть параметры, которые повышают вероятность ответа в заданной схеме.

30:07.500 --> 30:09.660
Вот это хотелось вам показать.

30:10.380 --> 30:11.640
Следующая возможность...

30:11.640 --> 30:11.880
Вот вопрос.

30:12.720 --> 30:21.020
Вот если она выдает текст пошагово и каждый раз оценивает наиболее вероятное следующее слово,

30:21.540 --> 30:23.240
то как она присваивает оценку?

30:23.240 --> 30:25.660
Вот здесь вот скоринг, который здесь 7, 6, 5.

30:26.020 --> 30:28.460
То есть это же не просто следующее слово с некой вероятностью.

30:28.600 --> 30:32.980
Как оно генерирует в сравнении друг с другом, тем более, эту оценку,

30:33.000 --> 30:34.280
если она движется по порядку?

30:34.780 --> 30:36.300
Сначала написала же первую строчку, правильно?

30:36.380 --> 30:37.320
Потом вторую, потом третью.

30:37.360 --> 30:40.080
Она не осмысляет текст целиком, она же генерирует его поэтапно.

30:41.540 --> 30:44.240
Как можно оценить в сравнении, если идет вывод поэтапный?

30:44.240 --> 30:49.820
Как можно оценить в сравнении, если она идет поэтапно?

30:49.820 --> 30:55.840
Ну, смотрите, вот в контексте, да, у нас там был черный G6,

30:56.320 --> 31:02.800
потом оно сгенерировало белые D4, королевская индийская защита,

31:03.440 --> 31:07.360
и она принимает, что если у нас там был, значит, черный G6,

31:07.600 --> 31:12.800
у нас белые ходят D4, значит, вероятность успешного исхода 7.

31:12.800 --> 31:16.280
Ну, вот так это как-то происходит.

31:19.600 --> 31:24.940
Ну, и смотрите, вот здесь эта вероятность, да, это, вот, score 7, да,

31:25.100 --> 31:29.600
это, ну, немного не та вероятность, по которой мы до этого говорили.

31:29.720 --> 31:33.160
То есть, скажем так, та таблица, которая была, вот,

31:33.420 --> 31:37.540
когда она сгенерировала семерку, да, у нее были какие-то еще другие альтернативы.

31:38.380 --> 31:41.820
Там семерка, тройка, единица, например, там.

31:42.700 --> 31:47.040
Для бета, да, была нулевая вероятность.

31:47.940 --> 31:54.260
И вот оно оценило, что в этой последовательности черные G6, белые D4,

31:54.640 --> 31:58.200
значит, оценка исхода, какие поставили задачу, 7.

31:59.040 --> 31:59.840
Вот я так понимаю.

32:01.600 --> 32:07.560
Но механизм генерации самой цифры 7 – это тоже наиболее вероятностное следующее слово.

32:08.420 --> 32:08.620
Да.

32:08.620 --> 32:15.460
Просто внутри, вот, вычисление этой вероятности следующее слово в данном случае было неким сложным,

32:15.740 --> 32:18.600
вот, более интеллектуальным, можно так сказать.

32:20.560 --> 32:22.100
Более интеллектуальным, чем что?

32:23.240 --> 32:27.680
Ну, если мы говорим про то, что нам набирается ответ, там, какое-то предложение, да,

32:27.840 --> 32:32.060
например, мы поздоровались, и оно набирает в ответ, там, предложение «Привет, как дела?».

32:32.060 --> 32:38.800
А здесь ему нужно оценить несколько вариантов развития событий, каким-то образом проранжировать их,

32:39.240 --> 32:42.900
определить, что вот этот вариант, там, наиболее приоритетный.

32:42.900 --> 32:47.080
Или я неправильно понимаю.

32:47.080 --> 32:50.800
Он же, они не могут объяснить, мы не знаем, как они приходят к этому стучению.

32:50.800 --> 32:57.220
А мы же раньше смотрели, коллеги, мы же раньше смотрели, у нас была табличка, там, таблица амбендингов и расчет вероятности,

32:57.320 --> 33:03.940
и быстрее всего просто с этой таблички, там, берутся, там, первые три ближайших, и все, и, собственно говоря, ну, я так понимаю.

33:03.940 --> 33:11.740
Ну, вот, вы считаете, что там внутри у нее шахматная программа, которая вот сделала функциональные блоки, да,

33:11.860 --> 33:17.560
и вот сгенерировала вот эти вот партии, да, как-то их там отранжировала.

33:18.700 --> 33:21.460
Вы считаете, что там есть такие объектные модели внутри у нее?

33:22.260 --> 33:30.360
Вот, насколько я знаю, нет, нет, просто ее много раз тренировали, там, черная G6, белые сходили D4,

33:30.360 --> 33:38.400
скор был равен, там, 7, если после этого белые сходили, там, D3, да, то скор был равен 2.

33:39.660 --> 33:47.440
Ну, я же наверняка скажу еще шахматные партии все, и видно сайты шахматные, там уже тоже в G7 все это хранится.

33:48.240 --> 33:50.600
Вот так вот оно и вываливает нам.

33:51.400 --> 33:59.440
Ну, вот у меня тоже такое впечатление, что внутри там механизм продолжения, механизм комплишена внутри.

34:00.360 --> 34:14.860
Так, следующая более сложная архитектура, да, называется вызов, вызов тулов или функций.

34:15.960 --> 34:24.360
Значит, теперь мы не просто, то есть структурный вывод, он работал так, что мы определяем тип возвращаемого результата, да,

34:24.360 --> 34:28.080
return мы теперь не текст, а теперь у нас надо в JSON схему это все сгенерировать.

34:28.780 --> 34:31.300
А здесь более сложный механизм.

34:31.400 --> 34:39.040
Смотрите, наверное, очевидно, что на JSON мы можем описать интерфейс нашего тула, да, или нашей функции.

34:39.140 --> 34:44.180
Например, здесь описана наша бизнес-система, которая возвращает статусы платежей.

34:44.180 --> 34:53.800
Так, здесь написано, что это функция, в JSON начинается функция, у нее есть имя, у нее есть описание, у нее есть параметры.

34:53.920 --> 35:00.000
Параметры, оно принимает transaction ID, и transaction ID это идентификатор транзакции, он обязательный.

35:00.100 --> 35:05.260
То есть здесь вот в описании этой схемы тоже не должно быть ничего, так оно очевидно, да,

35:05.260 --> 35:12.260
вот у нас есть такое описание тула нашей системы, интерфейса.

35:12.400 --> 35:18.760
Теперь я, значит, изображаю это в виде диаграммы последовательности.

35:18.760 --> 35:26.060
Смотрите, пользователь в чат пришел и спрашивает наше приложение, что с моим заказом T1001.

35:27.940 --> 35:28.260
Так.

35:35.260 --> 35:44.360
Да, наше приложение, значит, отправляет этот вопрос в языковую модель, вот, и добавляет описание тула,

35:44.580 --> 35:50.460
что у нас есть тул, который по номеру транзакции выдает статусы платежей.

35:50.780 --> 35:58.360
Ну, поскольку языковая модель, когда ее тренировали, она была совершенно, ну, она не могла быть подготовлена для ответа на вопрос про 1001 транзакцию, да,

35:58.360 --> 35:59.820
которая хранилась только у нас.

35:59.820 --> 36:02.320
Дальше.

36:04.920 --> 36:11.020
Языковая модель отвечает не, не знаю про языковую, ой, про статус транзакции, да,

36:11.160 --> 36:16.640
она отвечает специальным ответом в специальном формате, возвращает вызов тула.

36:17.480 --> 36:23.860
Это, в общем, инструкция, что я знаю, что у тебя есть такой тул в вызове,

36:23.860 --> 36:29.300
это тул, который возвращает статусы платежей с аргументом T1001.

36:29.300 --> 36:38.000
То есть она, пользовательский запрос на естественном языке, преобразовала вот в структурный вид вызов нашего интерфейса.

36:40.000 --> 36:45.920
Дальше. Ну, очевидно, что получив такой структурный запрос, мы можем его легко исполнить,

36:46.400 --> 36:53.400
так, поскольку у нас эти мэппинги наших интерфейсов, они на интерфейсы наших систем, внутренних платежных, да,

36:53.400 --> 36:57.020
они у нас очевидны и легко реализуемы.

36:57.500 --> 37:01.260
Нам здесь, видите, в этой архитектуре нам не нужно регэкспами, да,

37:01.600 --> 37:05.760
парсить пользовательские запросы, да, чем мы, наверное, занимались в предыдущее время.

37:06.300 --> 37:10.140
Мы легко из нашей базы достали статус и вернули его.

37:10.300 --> 37:15.140
Вот. И с возвратом этого статуса здесь вот интересно, нельзя его просто вернуть в языковую модель,

37:15.140 --> 37:18.280
потому что языковая модель — это все-таки стейтлесс механизм, да,

37:18.340 --> 37:23.140
я уже говорил, что на каждый запрос мы передаем ей всю историю нашего диалога,

37:23.960 --> 37:28.600
чтобы в ответ она у нас поддерживала историю диалога, скажем так.

37:29.120 --> 37:32.140
Теперь мы передаем ей всю последовательность сообщений.

37:33.260 --> 37:38.740
Исходный запрос пользователя, тот тул, который мы объявили, что у нас есть,

37:38.740 --> 37:53.060
тот запрос на вызов тула, который она нам возвращала,

37:53.560 --> 37:57.760
и третий — это результат вызова нашей системы,

37:57.820 --> 38:02.380
то есть, собственно, в структурном виде мы сообщаем, что статус был оплачен.

38:02.380 --> 38:08.560
И она на естественном языке формирует пользователю ответ о том,

38:08.720 --> 38:10.860
что ваша транзакция в таком-то статусе.

38:12.080 --> 38:16.660
Вот так работают там чаты пользовательской поддержки.

38:18.000 --> 38:22.400
И, в общем-то, все на этом механизме.

38:22.500 --> 38:27.020
И это вот одна из последних фишек, которая появилась в этой области.

38:27.800 --> 38:30.380
Как вы понимаете, очень мощный инструмент по интеграции

38:30.380 --> 38:34.700
интеграции наших корпоративных систем с языковыми моделями,

38:35.540 --> 38:39.000
которые при их создании совершенно не знали про нашу систему.

38:40.420 --> 38:41.820
Можно тоже здесь вопрос.

38:42.080 --> 38:49.540
Вот если мы возьмем ситуацию, что у нас значение статуса известно до обращения,

38:50.000 --> 38:51.940
и мы используем языковую модель просто для того,

38:52.020 --> 38:54.800
чтобы сообщить на естественном языке пользователю опрос статус.

38:54.800 --> 38:59.600
Мы же можем дернуть простую опишку, получить это значение,

38:59.740 --> 39:05.180
передать его в контекст и сказать, что у нас есть статус транзакции такой-то.

39:05.420 --> 39:08.960
Пожалуйста, скажи пользователю, что его транзакция в таком-то статусе.

39:09.280 --> 39:10.260
И она вернет этот ответ.

39:10.900 --> 39:13.820
Если конкретно кейс, который мы сейчас на примере решаем.

39:13.820 --> 39:16.720
То есть взять и уже готово отдать.

39:17.400 --> 39:18.080
Да, да, да.

39:18.240 --> 39:23.420
Ну, смотрите, здесь вот я вернусь к той проблеме, да,

39:23.640 --> 39:27.840
что вы говорите, что получив вот этот вот запрос от пользователя,

39:28.480 --> 39:30.180
а мышку мою видно, кстати?

39:31.160 --> 39:31.780
Да, да, видно.

39:32.300 --> 39:33.120
Мышку видно, так.

39:33.320 --> 39:33.800
Это хорошо.

39:34.380 --> 39:37.540
Да, вы говорите, что мы получили запрос от пользователя, да,

39:37.660 --> 39:40.720
и давайте сразу вот из системы получим статус.

39:40.720 --> 39:45.400
Ну, я говорю, что здесь вам нужны как бы регэкспы, да,

39:45.440 --> 39:49.080
чтобы понять, что пользователи интересуются статусом такого-то заказа.

39:49.160 --> 39:52.460
Но, допустим, у вас этот вот PaymentsDB, он, допустим, небольшой.

39:53.420 --> 39:56.980
И действительно, допустим, у вас всего там 10 заказов

39:56.980 --> 40:03.480
и 10 статусов этих заказов вы можете добавить это отдельным сообщением.

40:04.100 --> 40:06.840
Может даже, ну, наверное, и пользовательским или системным.

40:06.840 --> 40:11.880
То есть это будет выглядеть как ответь на вопрос пользователя,

40:12.240 --> 40:19.300
учти, что он может спрашивать про один из этих 10 транзакций, да,

40:19.480 --> 40:22.540
и эти транзакции находятся в соответствующих статусах.

40:22.640 --> 40:24.720
Это оплачено, это не оплачено, это не оплачено.

40:24.780 --> 40:27.580
И потом добавить в конец вопрос пользователя.

40:28.280 --> 40:31.160
Ну, и здесь языковая модель, получив этот вот контекст,

40:33.620 --> 40:36.120
может вполне ответить на этот вопрос.

40:36.840 --> 40:41.000
А если тогда взять исходный кейс и чуть расширить?

40:41.080 --> 40:43.160
Например, мы говорим про чат поддержки,

40:43.280 --> 40:45.040
где пользователь может спросить про транзакцию,

40:45.120 --> 40:48.880
а еще может спросить, я не знаю, там, ну, стоимость актуальную,

40:49.240 --> 40:51.560
рубля, например, на межбанке.

40:52.380 --> 40:56.780
И тогда я должен два разных тула передать, наверное, да,

40:56.820 --> 40:59.440
или как вот устроено, когда у нас ветвление по вопросам идет.

40:59.440 --> 41:05.100
Да, вот именно, что если мы берем на прямолинейный подход,

41:05.260 --> 41:09.160
просто вызывать языковую модель и передавать ей все наши данные в контексте,

41:09.220 --> 41:11.800
да, то они рано или поздно там просто не влезут туда, да.

41:12.220 --> 41:14.440
А здесь, видите, мы передаем tools, а это массив.

41:14.440 --> 41:20.120
И вот в этом ключевая мощность, что можно много этих tools передать.

41:21.420 --> 41:25.360
Языковая модель разберет вопрос пользователя на естественном языке

41:25.360 --> 41:30.160
и вызовет нужный из этих тулов и передаст туда нужные аргументы

41:30.160 --> 41:33.440
из естественного вопроса.

41:33.440 --> 41:37.740
А как она матчит, какой tool нужный?

41:39.460 --> 41:43.440
Насколько я понимаю, по description, вот мы когда описываем...

41:45.360 --> 41:48.900
То здесь есть тоже некоторая возможность ошибки.

41:50.280 --> 41:55.520
Если она вдруг внезапно заматчит не тот tool, почему то?

41:56.580 --> 41:57.840
Да, к сожалению.

41:57.840 --> 42:03.640
Так, у меня перестали переключаться слайды, я перезагружу это окошко.

42:04.800 --> 42:07.380
Ну, вероятность ошибочной классификации есть всегда.

42:07.600 --> 42:10.960
И в таком случае, да, действительно, он попытается ответить мне на тот вопрос,

42:11.780 --> 42:15.840
на неправильный, с использованием неправильного tool и даст...

42:16.540 --> 42:21.380
Ну, соответственно, он и ответ сформулирует так, как он понял вопрос.

42:21.500 --> 42:24.900
И пользователь уже по контексту поймет, ну, начнет детализировать,

42:24.900 --> 42:28.860
уточнять свой вопрос, что он, собственно говоря, имел в виду.

42:28.960 --> 42:33.000
Либо, может быть, в системе не очень корректно написано описание.

42:33.100 --> 42:37.020
Вот тот контент, который там у Миши был рядом с tool'ом описан,

42:37.020 --> 42:44.020
это то, почему идет классификация для вызова этого конкретного tool'а.

42:47.220 --> 42:48.220
То есть, да, вот этот вот...

42:48.220 --> 42:49.260
Вот этот description, понятно.

42:50.680 --> 42:51.800
Как бы по-нибудь, может ошибиться?

42:51.800 --> 42:53.520
Это основа для классификации. Конечно, может.

42:53.520 --> 42:56.540
Мне недавно описали в новостях,

42:56.720 --> 42:59.540
с дисконтом большими билетами на самолет подавали.

43:00.820 --> 43:01.920
С помощью такой системы.

43:04.820 --> 43:09.240
Ну, я для этого и показывал вот эту исходную логику работы системы,

43:09.280 --> 43:14.220
что она в своем, внутри, в ее сердце, она вероятностная.

43:15.060 --> 43:18.320
Это отличает ее от систем, которые мы строили до этого.

43:18.640 --> 43:20.700
Squirrel Select, он как бы Squirrel Select.

43:20.700 --> 43:24.540
Да, он не может куда-то не туда попасть.

43:25.560 --> 43:26.640
А тут, к сожалению, так.

43:27.720 --> 43:30.300
Так, теперь к физике, да, перейдем.

43:30.420 --> 43:33.220
К реализации инфраструктуры.

43:34.160 --> 43:36.720
У нас есть программа обеспечения.

43:38.000 --> 43:42.180
Значит, в предыдущей гемке мы использовали сервис компании MiStral.

43:42.300 --> 43:44.080
Она предоставляла эти сервера.

43:44.080 --> 43:47.840
Но как только мы начинаем работать с конфиденциальной информацией,

43:47.980 --> 43:50.660
нам это интересно у себя в периметре запускать.

43:51.760 --> 43:56.060
И для этого нам нужны инференс сервера.

43:56.600 --> 44:02.060
Отдельное программное обеспечение, которое выполняет эти языковые модели.

44:02.060 --> 44:06.240
Здесь сам термин инференс, да, наверное, важен.

44:07.100 --> 44:08.160
Инференс почему важен?

44:08.220 --> 44:10.820
Потому что вот в системах предыдущего поколения, да,

44:10.880 --> 44:14.360
со Squirrel серверами не было термина инференс.

44:14.880 --> 44:16.480
А здесь инференс – это очень важно,

44:16.620 --> 44:20.080
потому что в языковых моделях здесь тренировка этих моделей,

44:20.140 --> 44:23.060
она сильно отличается от этого вывода инференса.

44:24.000 --> 44:29.160
Ну и тут даже на русском языке сложно подобрать термин, да,

44:29.160 --> 44:31.600
вот вывод языковой модели, генерация значения.

44:32.160 --> 44:32.640
Ну, непонятно.

44:32.740 --> 44:35.220
В общем, есть отдельная задача – инференс.

44:36.000 --> 44:39.200
Ей отдельно надо заниматься, чтобы оно быстро работало.

44:39.660 --> 44:41.260
И для этого есть программное обеспечение.

44:42.100 --> 44:45.000
Здесь у меня перечислены VLM и Triton.

44:45.160 --> 44:49.740
Это наиболее частые реализации, которые вы можете встретить.

44:49.740 --> 44:55.700
В комментариях к этому слайду перечислены еще некоторые сервера,

44:55.800 --> 44:57.240
которые вы можете встретить.

44:57.240 --> 44:59.300
Опять же, важно это вам в том случае,

44:59.420 --> 45:03.720
если вы будете заниматься запуском этих моделей у себя в периметре.

45:04.460 --> 45:06.280
Также упомянуты библиотеки.

45:06.720 --> 45:10.120
Библиотеки – это менее функциональная штука, чем сервер.

45:10.300 --> 45:12.900
Оно предоставляет какое-то вычислительное ядро,

45:13.280 --> 45:15.140
которое исполняет сам код модели.

45:15.240 --> 45:18.020
Но его еще надо вызвать по сети.

45:19.180 --> 45:21.360
В библиотеках тут у меня упоминается Lama.cpp.

45:21.620 --> 45:24.160
Это прям часто упоминается она.

45:24.160 --> 45:27.740
Lama.cpp упоминается, потому что она поддерживает Google-формат.

45:28.080 --> 45:33.780
А Google-формат знаменит тем, что он может работать без дорогого железа видеокарт.

45:33.980 --> 45:36.600
Он подходит для домашнего прототипирования.

45:37.500 --> 45:44.720
Он может откладывать вычисления полностью или частично на обычный процессор.

45:44.720 --> 45:48.880
То есть он может отсчитывать эти модели на обычных процессорах.

45:50.260 --> 45:54.260
Поэтому не знаю, насколько это актуально для продакшена,

45:54.500 --> 46:00.840
но для экспериментов прототипов вот эти перечисленные сервера,

46:00.880 --> 46:03.720
они прям часто используются и упоминаются.

46:04.100 --> 46:10.600
А Lama, LmStudio, Local.ai – они все используют Lama.cpp и формат Google.

46:10.600 --> 46:16.400
Ну и, наверное, основная библиотека, на которой выполняются все эти исследования,

46:16.560 --> 46:18.160
это Hagen-Face Transformers.

46:18.680 --> 46:22.820
Это вот эталон функциональности для языковых моделей.

46:23.180 --> 46:27.960
Тоже вам может попасться, чтобы было понятно, что за трансформеры.

46:27.960 --> 46:35.720
Собственно, задачу какую решает Inflame Server?

46:38.240 --> 46:40.540
Задача… Так, вот давайте сейчас…

46:40.540 --> 46:41.840
Давайте. Прекрасный вопрос.

46:43.280 --> 46:46.960
Так, мы до этого…

46:46.960 --> 46:54.580
У нас лабораторная была с использованием сервиса Mistral.

46:54.780 --> 46:57.760
Теперь представим, что это происходит в периметре компании.

46:58.060 --> 47:00.880
И здесь у вас MLOps, специальный инженер,

47:01.020 --> 47:07.060
который специализируется на запуске этих языковых моделей,

47:07.420 --> 47:09.500
запустил для вас Inference Server.

47:09.500 --> 47:13.600
Например, вот такой командой VLM, серв, и передал имя модели.

47:13.600 --> 47:17.840
Она, значит, Inference Server стартует.

47:18.400 --> 47:21.380
Inference Server сам по себе небольшой, в нем этой модели нет.

47:22.960 --> 47:26.600
Он получает ее обычно из репозитория моделей.

47:27.500 --> 47:29.540
В базовом варианте, в самом начальном,

47:29.840 --> 47:34.700
все эти Inference Server выкачивают по запросу эти модели с Hagen-Face.

47:34.700 --> 47:40.760
Hagen-Face – это такой, как своеобразный GitHub для моделей машинного обучения.

47:40.760 --> 47:45.120
Там понятно, что мы про продакшен не говорим.

47:45.220 --> 47:47.660
В продакшене эта модель уже должна быть,

47:47.880 --> 47:52.980
но вот базовый вариант, вот он выглядит примерно так.

47:55.000 --> 47:59.720
Теперь вы можете засчитать это за ответ на предыдущий вопрос?

47:59.720 --> 48:06.940
Некий контейнер для моделек, деплоиванных локально, правильно?

48:08.480 --> 48:11.880
Да, это контейнер для моделей, да.

48:13.320 --> 48:16.740
С Hagen-Face мы получаем большие файлы, мы их, кстати, посмотрим.

48:16.880 --> 48:19.400
То есть эти файлы, они выкачиваются Inference Server,

48:19.400 --> 48:20.540
кладутся на диск.

48:21.100 --> 48:24.380
Дальше вот библиотека, например, Hagen-Face Transformers,

48:26.420 --> 48:33.240
читает эти файлы для того, чтобы сгенерировать ответ на запрос

48:33.240 --> 48:34.880
вот этого чат-комплишена,

48:35.800 --> 48:37.820
разобрать вот эти вот последовательные сообщения,

48:37.820 --> 48:40.460
втолкать их в файлы,

48:41.960 --> 48:45.360
использовать виса для того, чтобы сгенерировать следующее слово

48:45.360 --> 48:49.540
и это на этот запрос.

48:50.460 --> 48:50.940
Как-то так.

48:51.780 --> 48:54.000
А можно уточнение?

48:54.160 --> 48:58.480
Вчера нам показывали курсор, так называемый.

48:59.180 --> 49:03.460
Он был в идее, но это какая-то отдельная программулина.

49:03.760 --> 49:06.760
Она тоже пользовалась разными модельками, могла.

49:06.760 --> 49:09.720
Но я так понимаю, там просто какой-то API пользовалась

49:09.720 --> 49:11.700
к установленным где-то моделькам.

49:11.820 --> 49:15.220
А здесь прямо, условно говоря, условный докер,

49:15.380 --> 49:18.300
который умеет эту модельку ранить в себе.

49:19.180 --> 49:20.100
Вот этот VLLM.

49:21.780 --> 49:23.540
Да, да, да, правильно.

49:24.120 --> 49:25.200
Хорошо, спасибо.

49:25.880 --> 49:28.280
Ну, курсор – это клиентское приложение,

49:28.380 --> 49:31.420
это вот аналог вот этого зелененького свитка.

49:32.700 --> 49:36.300
И да, он по умолчанию подключается ко всяким сервисам,

49:36.300 --> 49:39.380
там OpenAI, Мистраль может подключиться,

49:39.980 --> 49:41.240
отсылает туда запросы.

49:41.360 --> 49:44.360
Ну и потом возникают вопросы безопасности.

49:44.600 --> 49:47.120
Можно ли какой-то код отсылать за периметр?

49:48.200 --> 49:50.180
И если у нас такой вопрос есть,

49:50.260 --> 49:52.720
нам приходится это в периметре организовывать

49:52.720 --> 49:54.720
инференс языковых моделей.

49:54.920 --> 49:56.300
И выполняется это примерно так.

49:56.300 --> 50:03.900
Можно ли еще сделать такое обобщение,

50:04.040 --> 50:05.460
аналогию некую провести,

50:05.940 --> 50:07.840
что у нас есть моделька,

50:07.940 --> 50:10.020
это чистая математика внутри себя, да,

50:10.500 --> 50:12.160
но есть стандартные методы,

50:12.320 --> 50:14.500
как вот сообщение должно поступить на вход,

50:14.640 --> 50:15.880
мы его должны там на слова,

50:16.140 --> 50:18.240
на токены разбить каким-то образом, да.

50:18.520 --> 50:19.720
Это некая стандартная вещь.

50:19.880 --> 50:21.460
Примерно так же, как у нас есть, допустим,

50:21.520 --> 50:24.800
сервера приложений для наших бизнес-каких-то задач, да,

50:24.800 --> 50:26.560
то есть мы делаем условный варник,

50:26.940 --> 50:28.500
деплоим его на серверы приложений,

50:28.940 --> 50:32.800
и вот стандартные запросы по HTTP-запросам,

50:33.580 --> 50:37.520
по выделению необходимого количества экзекьюторов на них,

50:38.040 --> 50:40.460
это те задачи, которые решает, собственно,

50:40.600 --> 50:41.500
вот этот сервер приложений,

50:41.620 --> 50:43.540
это вот VLM-сервер, да,

50:44.120 --> 50:46.300
а непосредственно варник, бизнес-логика,

50:46.300 --> 50:50.300
это вот в данном случае моделька.

50:51.240 --> 50:53.520
Ну, очень, ну, как бы...

50:53.520 --> 50:54.440
Очень натянута, да?

50:54.800 --> 50:57.800
Да-да-да, это можно сделать такую аналогию,

50:57.900 --> 50:59.060
но на очень высоком,

50:59.260 --> 51:04.220
очень-очень не вдаваясь в детали,

51:04.460 --> 51:05.240
потому что мы,

51:05.740 --> 51:08.420
если мы чуть-чуть попытаемся посмотреть в суд того,

51:08.520 --> 51:09.340
что происходит,

51:09.440 --> 51:10.880
там очень много противоречий,

51:11.080 --> 51:13.280
и вот то, что две эти схемы не сойдутся.

51:13.920 --> 51:14.860
Хорошо, пойдем, спасибо.

51:14.980 --> 51:17.040
Чти так, что запросы летят, да,

51:17.480 --> 51:18.700
но немного по-другому.

51:19.660 --> 51:20.320
Я даже не знаю,

51:20.400 --> 51:23.420
но все-таки это больше похоже на отдачу контента,

51:23.420 --> 51:27.260
да, вот это как бы кэширующий сервер, что ли.

51:27.680 --> 51:30.620
Ну, или считать, это как бы пережималка, да,

51:30.980 --> 51:33.600
конвертер видео, наверное, вот так можно сказать, да.

51:34.060 --> 51:36.960
Он по запросу получает видео с какого-то репозитория,

51:37.040 --> 51:39.300
а потом пережимает их под разные расширения.

51:39.760 --> 51:41.060
Вот, наверное, вот это ближе.

51:42.180 --> 51:42.480
Понял.

51:42.480 --> 51:45.100
Но пережимать же он может разными кодеками, да?

51:45.520 --> 51:46.080
Да, да, да.

51:46.580 --> 51:50.580
И кодеки, они уже должны быть как библиотека в inference сервере.

51:50.700 --> 51:52.180
То есть здесь еще, по идее,

51:52.260 --> 51:55.260
вот в inference сервере должны быть разные кодеки.

51:55.320 --> 51:58.000
Мы сейчас чуть-чуть про это посмотрим, да?

51:58.560 --> 51:58.980
Хорошо.

51:59.500 --> 52:02.440
Как мы будем выбирать языковую модель, да?

52:02.440 --> 52:05.140
Языковую модель мы выбираем по поставщику,

52:05.280 --> 52:08.860
ее названию, там, типсик сделал какую-то знаменитую модель,

52:09.020 --> 52:10.760
или там, квен сделал модель.

52:10.860 --> 52:13.460
Вот мы захотим ее получить, будем ее искать.

52:14.300 --> 52:17.200
Дальше модель должна быть совместима с нашим inference сервером.

52:17.560 --> 52:18.960
В inference сервере должен быть код,

52:19.020 --> 52:21.620
который может выполнять эту модель.

52:22.120 --> 52:24.340
Мы сейчас это увидим, как выбирается.

52:24.440 --> 52:29.080
В названиях модели всегда есть число, которое заканчивается B, да?

52:29.080 --> 52:32.480
А старые модели, они заканчивались на M,

52:32.760 --> 52:35.900
это были миллионы, а B это миллиарды,

52:35.960 --> 52:37.720
это миллиарды параметров.

52:38.320 --> 52:40.500
Ну и вот актуальное значение,

52:40.560 --> 52:42.480
которое мы можем у себя в периметре запускать,

52:42.540 --> 52:44.700
это 3B, 7B, вот такие.

52:45.840 --> 52:48.060
Огромные модели, там, 400B есть,

52:48.180 --> 52:50.000
но это там только мета,

52:50.120 --> 52:53.780
только Zuckerberg может себе позволить такое запустить.

52:54.320 --> 52:57.440
Вот на практике мы работаем с какими-то единичными B,

52:57.660 --> 52:58.680
миллиардами параметров.

52:58.680 --> 53:01.600
Параметры определяют размер файлов моделей.

53:02.880 --> 53:04.180
Из того, что мы смотрели,

53:04.360 --> 53:07.940
как логически эти модели генерируют текст,

53:08.500 --> 53:11.160
очень чувствительный параметр — это длина контекста.

53:11.300 --> 53:12.540
То есть длина контекста,

53:12.700 --> 53:15.140
которую он воспримет для генерации следующего слова,

53:15.220 --> 53:16.560
чтобы понимать зависимости,

53:16.980 --> 53:21.960
какие проходят между генерируемыми словами.

53:21.960 --> 53:27.460
Начиналось это все с тысяч токенов.

53:27.780 --> 53:30.460
Сейчас актуальные значения — это значит,

53:30.900 --> 53:32.460
заявляются миллионы,

53:32.580 --> 53:33.880
миллионный контекст,

53:34.020 --> 53:36.100
то есть там «Войну и мир» можно прочитать,

53:36.680 --> 53:39.040
но там всегда есть компромиссы по факту.

53:39.100 --> 53:40.500
Он вычитывает, конечно, миллион,

53:40.920 --> 53:44.880
но из этого миллиона он выбирает только часть токенов,

53:44.940 --> 53:46.720
которые действительно включаются в контекст,

53:46.720 --> 53:51.380
и по факту исследования показывают это 8-10К,

53:52.000 --> 53:53.440
актуальная длина контекста,

53:54.480 --> 53:56.440
а заявляется до миллиона.

53:56.540 --> 53:59.900
Это вот все в карточке модели может быть описано.

54:00.640 --> 54:03.220
Дальше модели можно пережимать

54:03.220 --> 54:04.720
и форматировать в разные форматы.

54:04.960 --> 54:07.880
Например, модели выходят в одном формате,

54:08.140 --> 54:09.820
Safe Tensors, например, PyTorch,

54:10.500 --> 54:13.340
а для любителей их пережимают в Google-формат.

54:13.340 --> 54:16.220
И если у вас дома вы захотите

54:16.220 --> 54:17.760
какой-то эксперимент поделать с моделью,

54:17.800 --> 54:20.220
вы будете искать модель в формате ГГУФа.

54:20.780 --> 54:24.180
Дальше уже идет еще другое направление —

54:24.180 --> 54:26.140
это квантизация, уменьшение разрядности

54:26.140 --> 54:27.760
каждого параметра модели.

54:28.020 --> 54:29.440
Я не помню, вчера про это говорили,

54:29.540 --> 54:31.260
сейчас тоже на это пытаемся посмотреть.

54:32.200 --> 54:34.240
И, значит, мы обычно ищем такой формат

54:34.240 --> 54:35.160
и такую квантизацию,

54:35.320 --> 54:37.360
которая бы влезла в нашу видеокарту.

54:37.360 --> 54:41.780
Ну, если мы используем специализированные

54:41.780 --> 54:44.260
графические ускорители.

54:44.780 --> 54:47.040
У моделей бывает разная лицензия.

54:47.360 --> 54:49.920
Какие-то модели скачиваются просто по запросу,

54:50.040 --> 54:51.740
какие-то требуют принятия лицензии,

54:51.920 --> 54:54.880
и мета может вам лицензию не дать

54:54.880 --> 54:56.200
на скачивание своей модели.

54:57.900 --> 54:59.300
У модели в карточке написано,

54:59.480 --> 55:00.720
какие языки она поддерживает.

55:00.860 --> 55:02.620
Ну, и это чувствительно, да,

55:02.700 --> 55:03.600
потому что нам нужно,

55:03.800 --> 55:05.020
чтобы они на русском отвечали,

55:05.020 --> 55:07.900
все популярные модели на русском отвечают,

55:08.020 --> 55:10.580
но этот русский там случайно попал в выборку.

55:10.880 --> 55:12.040
Это их не сильная сторона.

55:12.280 --> 55:13.940
Ну, а китайские модели, разумеется,

55:14.020 --> 55:14.820
сильны в китайском.

55:16.220 --> 55:20.380
И вот эти вот подножество поддерживаемых языков,

55:20.460 --> 55:23.540
оно входит в модель на самых ранних стадиях обучения,

55:23.620 --> 55:24.060
в притрении.

55:24.540 --> 55:26.720
Поэтому переделать модель на другой язык,

55:26.760 --> 55:28.080
как бы это очень сложно.

55:28.300 --> 55:29.900
Поэтому этот важный параметр.

55:30.600 --> 55:33.240
Дальше в названии модели

55:33.240 --> 55:35.000
есть еще вот такие суффиксы,

55:35.020 --> 55:38.140
называется Instruct, Chat, IT.

55:38.940 --> 55:41.720
IT – это сокращение от Instruct.

55:42.140 --> 55:45.620
Это возможность работать вот в модели

55:45.620 --> 55:46.640
нескольких сообщений

55:46.640 --> 55:48.520
и выполнять какие-то задания.

55:49.300 --> 55:51.920
Мне сейчас вот как бы будет сложно, да,

55:52.260 --> 55:56.060
объяснить отличия между базовой моделью

55:56.060 --> 55:59.240
и вот этим Instruct и Chat моделями, да,

55:59.240 --> 56:04.940
но можно вот остановиться на том,

56:05.020 --> 56:08.460
что нам для практических задач, да,

56:08.620 --> 56:11.040
нужна модель, которая вот Instruct и Chat.

56:11.440 --> 56:14.660
Это она прошла такой заключительный этап в FineTune.

56:14.780 --> 56:17.940
Она тренировалась на многих заданиях.

56:17.940 --> 56:20.620
Как-то не очень объяснил, да.

56:20.820 --> 56:24.100
Дальше у модели в карточке написано,

56:24.520 --> 56:27.100
поддерживает ли она структурный вывод

56:27.100 --> 56:30.960
и тулы или функции.

56:31.460 --> 56:32.980
Еще есть модели VL – это те,

56:33.060 --> 56:35.440
которые умеют картинки обрабатывать

56:35.440 --> 56:37.720
на вход или на выход.

56:37.720 --> 56:42.440
Например, вот как выглядит карточка модели.

56:43.240 --> 56:45.000
Похоже, с тем, что мы работали.

56:45.260 --> 56:47.880
Так, сейчас Мистраль у вас показывается, да,

56:47.960 --> 56:50.000
вот на сайте HagenFace

56:50.000 --> 56:53.220
есть вот организация Мистраль AI,

56:53.340 --> 56:55.400
у нее есть публикована модель.

56:55.600 --> 56:58.900
У модели, что тут вот, лицензия объявлена,

56:59.520 --> 57:03.100
описаны сервера, на которых она запущена.

57:03.100 --> 57:05.020
Вот VLM можно запустить.

57:05.940 --> 57:09.260
Это форматы, библиотеки,

57:09.360 --> 57:11.480
которые она поддерживает, языки описаны.

57:11.680 --> 57:14.200
Вот важно – генерация текста.

57:14.500 --> 57:16.260
Модели есть разные, мы чуть позже,

57:16.300 --> 57:19.000
если время у нас останется, остается.

57:19.740 --> 57:21.000
Эта модель генерирует текст.

57:21.180 --> 57:22.680
И здесь, смотрите, у некоторых моделей

57:22.680 --> 57:24.460
запущен чат.

57:25.380 --> 57:27.540
И с некоторыми можно даже здесь вот на сайте

57:27.540 --> 57:29.160
повзаимодействовать, посмотреть,

57:29.160 --> 57:31.360
как они примерно отвечают.

57:31.360 --> 57:34.740
Вот здесь на карточке написано,

57:35.120 --> 57:39.320
вот сразу написано, что разрядность у нее такая,

57:39.580 --> 57:43.100
на какой карте, с какой видеопамять ее можно запустить.

57:45.120 --> 57:45.800
Лицензия.

57:47.080 --> 57:49.640
Упоминается ее токенизатор,

57:49.980 --> 57:51.520
сколько токенов она поддерживает.

57:51.640 --> 57:53.840
Там дальше должно быть про тулы,

57:54.680 --> 57:56.180
про функции.

57:57.140 --> 57:58.780
Написано, как запустить ее,

57:58.840 --> 57:59.620
на каких серверах.

57:59.720 --> 58:01.480
То есть здесь вот довольно подробное описание,

58:02.600 --> 58:03.800
как вызывать ее в коде.

58:07.200 --> 58:09.760
Так, как запустить на АЛАМе,

58:09.900 --> 58:11.980
как найти даже квантизованную версию.

58:12.180 --> 58:15.200
Квантизованная, чтобы могла у вас запуститься

58:15.200 --> 58:17.020
на скромных ресурсах.

58:20.060 --> 58:21.940
Размер модели, параметры.

58:22.320 --> 58:24.200
Вот здесь, например, квантизация.

58:24.200 --> 58:26.840
То есть вся модель у нас не влезает.

58:27.040 --> 58:31.280
Мы идем и ищем ее квантизованную версию.

58:31.740 --> 58:35.280
По вот этой ссылке здесь много разных квантизаторов,

58:35.400 --> 58:38.640
но здесь много самодеятельных квантизаторов.

58:38.860 --> 58:41.300
И часто бывает, что квантизацию какую-то скачиваете,

58:41.420 --> 58:44.580
она маленькая, но генерирует какую-то билиберду.

58:44.580 --> 58:48.620
Ну, как-то так.

58:48.740 --> 58:50.660
Дальше, как эта модель выглядит?

58:51.660 --> 58:54.280
Здесь, на этой вкладке Files and Versions,

58:54.400 --> 58:56.900
можно увидеть вот большие файлы весов моделей.

58:57.300 --> 59:00.180
Собственно, вот то, что составляет ее знания.

59:01.340 --> 59:03.980
Очень важный файлик, вот конфиг.

59:03.980 --> 59:07.840
Здесь много что упомянуто.

59:08.200 --> 59:11.240
Например, ну, вот самый важный параметр – это вот архитектура.

59:11.360 --> 59:14.880
Здесь написано, что у этой модели вот такая архитектура.

59:15.900 --> 59:21.640
И здесь, если мы пойдем на один из популярных серверов VLLM,

59:22.060 --> 59:25.280
у него в документации описано, какие бывают архитектуры.

59:25.280 --> 59:31.560
Ну, и, в общем, резюме такое, что вот, используя всю эту информацию,

59:31.640 --> 59:36.300
да, мы подбираем модель, как она у нас, такая, чтобы она у нас запустила,

59:36.420 --> 59:37.460
отвечала то, что надо.

59:39.600 --> 59:42.360
Таким образом мы ищем наши модели.

59:44.440 --> 59:46.540
Так, завершается первая часть.

59:47.300 --> 59:50.360
Мы посмотрели, как вызвать модель на inference,

59:50.640 --> 59:52.940
посмотрели, как сделать пользовательский интерфейс,

59:52.940 --> 59:57.060
программный интерфейс, познакомились с двумя мощными возможностями.

59:57.160 --> 59:59.900
Это структурный вывод, вызов, тулов.

01:00:01.300 --> 01:00:04.860
Посмотрели, какие бывают сервера, библиотеки,

01:00:05.020 --> 01:00:06.940
где искать модели и какие модели бывают.

01:00:07.780 --> 01:00:08.980
Первая часть закончилась.

01:00:09.340 --> 01:00:10.600
Прошел час.

01:00:11.080 --> 01:00:12.660
Вопросы здесь есть какие-то?

01:00:16.400 --> 01:00:17.040
Нет.

01:00:17.220 --> 01:00:18.520
Поехали дальше.

01:00:18.820 --> 01:00:21.900
Вторая часть про эмбеддинги и другую модель.

01:00:22.940 --> 01:00:23.840
Модель BERT.

01:00:23.960 --> 01:00:25.720
Вот внизу написано, что она означает.

01:00:27.320 --> 01:00:30.260
Мы говорили до этого про GPT-модели.

01:00:30.340 --> 01:00:32.300
GPT-модели генерируют текст, да,

01:00:32.460 --> 01:00:33.880
и вот они стали популярны.

01:00:34.720 --> 01:00:39.160
С конца 2023 года прям поднялся большой хайп,

01:00:39.280 --> 01:00:46.460
а до этого, в 2018 году, да, был хайп на вот эти вот модели,

01:00:46.460 --> 01:00:52.580
потому что они очень сильно улучшили функциональность Google поиска.

01:00:52.740 --> 01:00:53.800
Вот оттуда пошел BERT.

01:00:54.720 --> 01:00:56.860
Значит, что делает BERT?

01:00:57.080 --> 01:01:03.340
Он получает на вход текстовую строку и на выходе формирует вектор.

01:01:03.840 --> 01:01:08.340
Вектор чисел, да, совершенно непонятный, что это за числа,

01:01:08.340 --> 01:01:12.200
но эти вектора обладают очень интересными свойствами,

01:01:13.760 --> 01:01:15.540
которые все используют.

01:01:18.240 --> 01:01:21.000
Так, как можно использовать?

01:01:21.100 --> 01:01:24.420
Например, можно использовать для поиска похожих текстов.

01:01:24.620 --> 01:01:28.300
То есть если у нас два текста написаны разными словами,

01:01:28.580 --> 01:01:30.700
но так получилось, что они про одно и то же,

01:01:31.480 --> 01:01:35.420
то у них значения этих векторов будут друг другу близки.

01:01:35.420 --> 01:01:38.760
Это вот такая простая идея, сильно не выдаваясь в математику.

01:01:39.940 --> 01:01:42.360
Так, это вот первое примитивное свойство.

01:01:43.100 --> 01:01:45.820
И, как вы понимаете, если мы можем установить,

01:01:46.100 --> 01:01:49.220
какие тексты имеют один и тот же смысл,

01:01:49.400 --> 01:01:52.480
это дает нам возможность поисковой системы.

01:01:52.780 --> 01:01:55.700
То есть у нас есть документы, у нас есть запрос,

01:01:56.240 --> 01:02:01.840
и если мы вот по этим моделям сравниваем запрос и документы,

01:02:01.840 --> 01:02:05.720
он позволит нам найти наиболее похожие релевантные документы.

01:02:06.100 --> 01:02:10.620
А это значит, что он позволит нам найти на нашей там вики,

01:02:10.900 --> 01:02:16.080
на нашем knowledge hub ответ на пользовательский вопрос,

01:02:16.300 --> 01:02:17.580
там, как уйти в отпуск.

01:02:17.580 --> 01:02:24.580
Это основная идея.

01:02:24.640 --> 01:02:28.340
Мы переходим ко второй лабораторке,

01:02:28.400 --> 01:02:30.400
как я их называю, ко второму воркбуку.

01:02:31.820 --> 01:02:33.860
Здесь мы используем тот же...

01:02:33.860 --> 01:02:37.020
Так, здесь я не совершу ошибку.

01:02:38.700 --> 01:02:39.580
Надо запустить...

01:02:40.320 --> 01:02:41.280
А, вот этот, да.

01:02:41.500 --> 01:02:44.800
Вот я запущу pip install очень долгий.

01:02:44.860 --> 01:02:46.120
Этот pip install очень долгий.

01:02:46.120 --> 01:02:49.120
Смотрите, здесь мы вызовем вот эти модели эмбеддингов,

01:02:49.880 --> 01:02:51.360
модели словарных вложений.

01:02:52.020 --> 01:02:54.280
Вот эти вектора, они называются эмбеддинги.

01:02:55.760 --> 01:02:58.840
А здесь мы определяем эти три секрета.

01:02:59.240 --> 01:03:01.720
Обратите внимание, что теперь у нас используется не модель,

01:03:01.800 --> 01:03:04.120
а эмб модель, модель эмбеддингов.

01:03:04.820 --> 01:03:08.520
Она вот не та, которая была в предыдущем упражнении.

01:03:09.280 --> 01:03:11.320
Также мы подключаемся к мистралю,

01:03:11.320 --> 01:03:15.760
передавая секретный ключ и URL.

01:03:16.120 --> 01:03:18.120
И вызываем эмбеддинги с помощью...

01:03:18.120 --> 01:03:21.600
Модель эмбеддингов с помощью вот этой функции,

01:03:21.740 --> 01:03:22.760
передавая текст.

01:03:23.500 --> 01:03:26.020
И смотрим, чем отвечает эта модель.

01:03:27.740 --> 01:03:29.760
А, ну это мы смотреть будем долго.

01:03:29.880 --> 01:03:32.880
Да, потому что sentence transformer ставится очень долго.

01:03:32.880 --> 01:03:39.180
Остался вывод с предыдущего раза.

01:03:39.500 --> 01:03:43.440
Смотрите, она выдаст нам 1024 дробных числа.

01:03:44.720 --> 01:03:48.880
Так эта модель работает, и это то, что я вам обещал на предыдущих слайдах.

01:03:48.880 --> 01:03:53.900
Оно ответит довольно быстро.

01:03:55.900 --> 01:04:01.980
Мы пока ждем того, как поставится библиотека sentence transformer.

01:04:02.220 --> 01:04:03.800
Что делает sentence transformer?

01:04:04.180 --> 01:04:06.500
Это библиотека на Python,

01:04:06.720 --> 01:04:11.580
которая позволяет вот эти модели эмбеддингов словарных вложений

01:04:11.580 --> 01:04:13.580
выполнять на...

01:04:13.580 --> 01:04:14.680
Ну, как бы локально.

01:04:14.900 --> 01:04:18.480
Ну, здесь локально тоже относительно.

01:04:18.600 --> 01:04:20.200
Мы здесь используем Google Collab

01:04:20.200 --> 01:04:24.180
для того, чтобы на каком-то Python узле

01:04:24.180 --> 01:04:25.640
выполнять вычисления.

01:04:27.580 --> 01:04:31.320
Ну, как бы это условно-локально,

01:04:31.440 --> 01:04:33.580
но это все равно не вызов API,

01:04:33.580 --> 01:04:38.720
потому что здесь мы вызывали API серверов у компании Mistral,

01:04:38.720 --> 01:04:42.020
а здесь мы вычисления векторов

01:04:42.020 --> 01:04:48.000
выполним на этом Google Collab ноде.

01:04:49.280 --> 01:04:49.520
Так.

01:04:50.580 --> 01:04:52.680
Ладно, оно пока ставится, оно поставится

01:04:52.680 --> 01:04:54.380
и начнет нам считать.

01:04:54.480 --> 01:04:58.620
Сейчас мы будем использовать эти вектора

01:04:58.620 --> 01:05:04.400
и посмотрим на их свойства.

01:05:04.520 --> 01:05:05.340
Смотрите, что я сделал.

01:05:05.420 --> 01:05:08.620
Я взял пять заголовков новостей,

01:05:09.440 --> 01:05:11.420
вот это вот...

01:05:11.420 --> 01:05:13.400
Нет, там даже их было три.

01:05:15.080 --> 01:05:16.560
В общем, идея такая.

01:05:18.860 --> 01:05:21.680
Один заголовок я повторил полностью,

01:05:22.380 --> 01:05:24.100
второй раз я переставил слова,

01:05:25.100 --> 01:05:28.020
да, а две следующих записи...

01:05:28.020 --> 01:05:30.280
А, да, то есть все было три новости.

01:05:30.380 --> 01:05:34.500
Я заменил одно слово и заменил его же на другое.

01:05:34.500 --> 01:05:39.160
Ну, причем у слова корабль есть немного другой смысл.

01:05:39.320 --> 01:05:41.720
Корабль — это военный, да, а судно — это гражданское.

01:05:41.780 --> 01:05:44.860
Вот танкер, он ближе к судну, чем к кораблю по семантике.

01:05:45.540 --> 01:05:49.200
Теперь мы будем считать разницу между этими векторами,

01:05:49.280 --> 01:05:51.200
поскольку значение этих векторов...

01:05:51.200 --> 01:05:57.280
А, вот, кстати, получился у нас вектор из sentence-трансформера.

01:05:58.120 --> 01:06:00.120
Так, да, все поставилось.

01:06:00.240 --> 01:06:02.020
Возвращаемся к исходным клеткам.

01:06:03.180 --> 01:06:05.420
Мистераль мы вызвали, передав ей строчку,

01:06:05.480 --> 01:06:07.980
и получили вот вектор дробных чисел.

01:06:07.980 --> 01:06:10.820
Здесь тысяча дробных чисел,

01:06:10.900 --> 01:06:13.140
совершенно непонятно, что это за дробные числа.

01:06:13.260 --> 01:06:13.620
Отлично.

01:06:14.300 --> 01:06:17.800
Теперь мы поставили и вызвали библиотеку sentence-трансформер,

01:06:17.940 --> 01:06:19.980
вызвали ее с вот такой моделью,

01:06:20.100 --> 01:06:21.140
с моделью mpnet,

01:06:21.280 --> 01:06:23.060
передали ту же строчку,

01:06:23.940 --> 01:06:24.980
и получили...

01:06:26.220 --> 01:06:28.940
Вот первое, что интересное происходит, да,

01:06:29.680 --> 01:06:31.780
когда вы будете самостоятельно это делать.

01:06:32.600 --> 01:06:33.860
Библиотеку мы создали,

01:06:33.860 --> 01:06:37.000
но при вызове ее происходит вот выкачивание с хагенфейса

01:06:37.000 --> 01:06:38.060
вот этой библиотеки.

01:06:38.500 --> 01:06:39.520
Здесь оно из хагенфейса,

01:06:39.720 --> 01:06:40.860
эти файлы получают,

01:06:40.940 --> 01:06:43.240
и прямо в анимации можно по этим прогресс-барам

01:06:43.240 --> 01:06:45.120
увидеть, как с хагенфейса

01:06:45.120 --> 01:06:47.200
оно веса модели получало,

01:06:47.300 --> 01:06:49.200
на хагенфейс можно сходить и посмотреть.

01:06:49.760 --> 01:06:52.380
Какие-то модели оно с хагенфейса вам не даст,

01:06:52.440 --> 01:06:54.860
если вы не авторизуете HF-токен

01:06:54.860 --> 01:06:56.320
и его здесь не передадите.

01:06:56.940 --> 01:07:00.180
Но, тем не менее, тоже тысячу дробных чисел,

01:07:00.280 --> 01:07:02.720
оно меньше, оно 700 с чем-то,

01:07:02.720 --> 01:07:06.920
модель MPNET использует вектора размером 768.

01:07:07.180 --> 01:07:10.280
Модели разные, у каждой своя разрядность этих векторов.

01:07:11.000 --> 01:07:13.120
Поскольку совершенно непонятно,

01:07:13.260 --> 01:07:14.260
что это за дробные числа,

01:07:14.360 --> 01:07:16.800
давайте между ними будем считать расстояние.

01:07:16.860 --> 01:07:19.080
Расстояние – это просто разница значений.

01:07:20.500 --> 01:07:21.960
Если мы между вот этими,

01:07:22.060 --> 01:07:25.340
мы получили 7 строчек, да,

01:07:26.500 --> 01:07:28.700
7 строчек, расположенных по строчкам,

01:07:28.920 --> 01:07:32.000
и те же 7 строчек по колонкам, да,

01:07:32.000 --> 01:07:33.900
и насчитаем квадратную матрицу.

01:07:34.100 --> 01:07:36.440
Это отношение каждой с каждым.

01:07:37.960 --> 01:07:38.680
Что мы увидим?

01:07:38.760 --> 01:07:42.800
Она ожидаемо, она будет иметь в диагонали нули,

01:07:43.000 --> 01:07:46.360
потому что каждая строчка эквивалентна сама себе,

01:07:46.800 --> 01:07:48.300
она посчитается в тот же вектор.

01:07:48.540 --> 01:07:50.420
Из того, что диагональ нулевая,

01:07:50.500 --> 01:07:51.400
можно сделать вывод,

01:07:51.400 --> 01:07:53.900
что эти дробные числа,

01:07:54.100 --> 01:07:56.040
они, на удивление, не случайные, да,

01:07:56.120 --> 01:07:57.540
все-таки в них какой-то смысл есть,

01:07:57.660 --> 01:08:02.040
потому что для эквивалентной строчки

01:08:02.040 --> 01:08:03.040
она возвращает ноль.

01:08:03.360 --> 01:08:04.260
Это же мы видим,

01:08:04.380 --> 01:08:06.940
если мы посмотрим на расстояние до второй строчки.

01:08:07.140 --> 01:08:08.120
Первая и вторая строчка,

01:08:08.180 --> 01:08:09.840
они совпадают по-символьно.

01:08:09.840 --> 01:08:11.940
Дальше, смотрите,

01:08:12.400 --> 01:08:15.920
та же самая строчка с переставленными словами

01:08:15.920 --> 01:08:18.180
немножко дальше уже отстоит,

01:08:18.800 --> 01:08:20.500
но и она отстоит дальше,

01:08:20.740 --> 01:08:24.560
чем замена на одно слово, да.

01:08:25.860 --> 01:08:27.940
Замена на корабль отстоит дальше

01:08:27.940 --> 01:08:29.520
от замены на судно.

01:08:29.840 --> 01:08:30.820
Я объясняю это так,

01:08:30.940 --> 01:08:34.080
что за кораблем есть военная коннотация,

01:08:34.080 --> 01:08:36.740
но все равно оно по смыслу

01:08:36.740 --> 01:08:38.780
не так далеко,

01:08:39.020 --> 01:08:40.660
как переставленные слова полностью.

01:08:41.080 --> 01:08:43.460
Это вот уже сложный момент.

01:08:43.740 --> 01:08:45.060
А вот эти вот две новости,

01:08:45.200 --> 01:08:47.320
они просто контрольные, да,

01:08:47.360 --> 01:08:49.240
они по смыслу сильно отличаются.

01:08:49.600 --> 01:08:51.580
Одна про санкции и газ,

01:08:51.660 --> 01:08:52.960
а другая про ключевую ставку,

01:08:53.040 --> 01:08:53.660
про финансы.

01:08:54.100 --> 01:08:55.540
Они, очевидно, отстоят дальше

01:08:55.540 --> 01:08:58.600
и вот разница в них больше, да,

01:08:59.560 --> 01:09:00.900
чем исходная строчка

01:09:00.900 --> 01:09:03.260
про транзитное судно.

01:09:04.080 --> 01:09:07.460
Примерно такое же поведение

01:09:07.460 --> 01:09:09.920
мы видим у модели MPNet,

01:09:10.560 --> 01:09:12.620
но видим, что актуальна, да,

01:09:12.860 --> 01:09:15.600
вот разница между векторами, да,

01:09:15.660 --> 01:09:17.180
но для этих моделей она разная,

01:09:17.260 --> 01:09:19.240
потому что это разные модели.

01:09:20.120 --> 01:09:24.360
Но общий смысл, он сохраняется, да,

01:09:24.440 --> 01:09:26.880
то есть чем дальше новость по семантике,

01:09:26.940 --> 01:09:28.720
да, тем больше эта разница.

01:09:29.540 --> 01:09:31.020
И отсюда можно сделать вывод,

01:09:31.020 --> 01:09:33.120
что эти модели,

01:09:33.120 --> 01:09:36.820
они позволяют искать похожие строки,

01:09:37.120 --> 01:09:38.960
тексты по смыслу,

01:09:39.400 --> 01:09:42.340
представляя их вот этими многомерными векторами.

01:09:43.420 --> 01:09:46.140
Здесь вот по этой вот математической билиберде

01:09:46.140 --> 01:09:47.220
есть какие-то вопросы?

01:09:48.060 --> 01:09:50.560
Понятно, зачем и как это вообще было?

01:09:50.560 --> 01:09:52.120
Продвинутая дистанция Ливенштейна, да?

01:09:52.120 --> 01:09:56.020
Ну, Ливенштейны, они по буквам считаются,

01:09:56.400 --> 01:09:57.960
а здесь мы по...

01:09:57.960 --> 01:09:59.280
Ну да, по словам.

01:09:59.660 --> 01:10:02.740
Ну вот, оно вот не Ливенштейном, видите.

01:10:03.600 --> 01:10:04.020
Нет, понятно.

01:10:04.020 --> 01:10:06.380
Ливенштейне мы можем потом сказать,

01:10:06.660 --> 01:10:09.560
что вот этот вот, да, компонент,

01:10:10.080 --> 01:10:12.320
это окончание, оно тут поменялось.

01:10:12.320 --> 01:10:16.900
А здесь очень сложно исходный текст

01:10:16.900 --> 01:10:20.200
соотнести с измерениями этих векторов.

01:10:22.540 --> 01:10:23.460
Но смысл есть.

01:10:24.460 --> 01:10:27.280
Еще раз, у нас здесь строчки и столбцы,

01:10:27.600 --> 01:10:31.120
это отдельные строки инпута, правильно?

01:10:31.940 --> 01:10:34.760
Да, ну это те же самые строки инпута.

01:10:34.820 --> 01:10:37.380
Я хочу вот эту строчку со всеми остальными сравнить.

01:10:40.360 --> 01:10:42.740
Ну тут интереснее поиграться с одним словом,

01:10:42.840 --> 01:10:44.020
на самом деле, в каждой строчке

01:10:44.020 --> 01:10:45.840
и посмотреть, как оно на это реагирует.

01:10:47.340 --> 01:10:50.320
Да, да, ну причем вот здесь уже видно,

01:10:50.480 --> 01:10:52.100
что вот на перестановку строк, да,

01:10:52.200 --> 01:10:54.420
вот эти две модели, они ведут себя по-разному.

01:10:54.900 --> 01:10:56.560
Но и, кстати, я потом пришел к тому,

01:10:56.680 --> 01:10:59.040
что MPNet, русский язык как-то не очень,

01:10:59.040 --> 01:11:01.480
я ее дисквалифицировал из следующей лабы.

01:11:02.420 --> 01:11:05.340
Прямо скажу, что-то MPNet прям популярное,

01:11:05.420 --> 01:11:06.400
много где встречается,

01:11:06.560 --> 01:11:09.780
но что-то у меня рак на MPNet не заработал, кстати.

01:11:11.780 --> 01:11:15.040
Так, мы посмотрели про P-encoder.

01:11:16.080 --> 01:11:18.900
Так, это у нас была лаба, которую мы сейчас прошли.

01:11:19.200 --> 01:11:19.400
Оба.

01:11:20.080 --> 01:11:21.400
Так, и мы переходим.

01:11:21.540 --> 01:11:24.140
Да, у нас 15 минут на RAG остается.

01:11:24.420 --> 01:11:24.860
Отлично.

01:11:25.660 --> 01:11:27.600
Retrieal Augmented Generation.

01:11:27.600 --> 01:11:30.320
Это мы возвращаемся вот к GPT-моделям,

01:11:30.400 --> 01:11:31.760
они генерируют текст, да.

01:11:32.680 --> 01:11:35.660
Retrieal Augmented, значит, улучшенное,

01:11:35.760 --> 01:11:38.520
дополненное, обогащенное поиском.

01:11:39.080 --> 01:11:40.720
Почему надо обогащать поиском?

01:11:40.840 --> 01:11:45.180
Потому что к GPT-модели этих OpenAI,

01:11:45.360 --> 01:11:46.040
Сэма Альтмана,

01:11:46.140 --> 01:11:47.680
они ничего не знают про наш бизнес.

01:11:48.260 --> 01:11:50.540
И как вот мы им предлагали, да, сейчас.

01:11:51.300 --> 01:11:54.260
А давайте в контекст к пользовательскому запросу

01:11:54.260 --> 01:11:56.440
добавим всю нашу небольшую таблицу.

01:11:56.440 --> 01:11:59.540
Вдруг у нас таблица ордеров небольшая, да.

01:11:59.600 --> 01:12:01.520
Давайте весь статус платежей туда добавим.

01:12:02.240 --> 01:12:05.660
И он сможет сгенерировать ответ на запрос пользователя.

01:12:05.660 --> 01:12:11.360
Так, как это теперь работает?

01:12:12.960 --> 01:12:14.780
Пользователь что-то у нас спрашивает, да.

01:12:14.860 --> 01:12:16.720
Мы вместо того, чтобы сразу генерировать,

01:12:17.040 --> 01:12:20.220
можем выполнить запрос в векторном хранилище.

01:12:20.300 --> 01:12:22.720
Векторное хранилище – это такая база данных,

01:12:23.220 --> 01:12:25.840
которая с помощью близости этих векторов

01:12:25.840 --> 01:12:30.480
извлекает релевантные, подходящие по смыслу документы.

01:12:30.480 --> 01:12:33.320
То есть, как это можно представить?

01:12:33.480 --> 01:12:37.360
Пользователь задает какой-то там вопрос про свой заказ.

01:12:39.220 --> 01:12:41.440
Допустим, мы по идентификатору заказа,

01:12:41.460 --> 01:12:43.900
ну, это структурный поиск, да,

01:12:43.960 --> 01:12:45.020
это не очень удачно.

01:12:45.120 --> 01:12:47.860
Но, в общем, допустим, мы передадим только документы,

01:12:47.980 --> 01:12:50.480
которые соотносятся с его заказом.

01:12:52.340 --> 01:12:55.620
Они все найденные документы,

01:12:55.760 --> 01:12:58.600
они все добавляются вот в контекст запроса.

01:12:58.600 --> 01:13:04.040
И мы отправляем это на генерацию ответа,

01:13:04.160 --> 01:13:05.600
и пользователь получает ответ,

01:13:06.880 --> 01:13:10.540
но уже ответ с тем контекстом,

01:13:10.880 --> 01:13:13.040
с контекстом нашей информационной системы,

01:13:13.200 --> 01:13:16.100
нашего предприятия, корпорации.

01:13:16.640 --> 01:13:18.620
Здесь еще добавлена одна стрелочка.

01:13:19.440 --> 01:13:22.980
Эмбединги, да, нам нужно будет из нашей системы

01:13:22.980 --> 01:13:25.920
у сервиса языковых моделей

01:13:25.920 --> 01:13:27.820
вот получать эти вектора.

01:13:28.600 --> 01:13:32.600
семантические вектора для наших документов.

01:13:34.780 --> 01:13:36.700
Так, и про это.

01:13:38.640 --> 01:13:40.540
Да, у нас третья лаба.

01:13:42.020 --> 01:13:44.860
Третья лаба, она такая же, как первая.

01:13:46.380 --> 01:13:47.740
Здесь ключи.

01:13:47.740 --> 01:13:52.080
Так, да, здесь тот же трюк, что надо запустить центр трансформер.

01:13:52.160 --> 01:13:53.220
Да, для третьей лабы.

01:13:53.300 --> 01:13:53.900
Что нам нужно?

01:13:54.920 --> 01:13:56.140
Там еще была ссылка.

01:13:57.040 --> 01:13:58.120
Она есть в чате.

01:13:58.780 --> 01:13:59.740
И есть...

01:14:01.120 --> 01:14:03.320
О, так, я сейчас отвечу на вопрос.

01:14:03.760 --> 01:14:04.780
Пока буду отвечать.

01:14:04.780 --> 01:14:08.780
Там есть задача, надо скопировать...

01:14:10.180 --> 01:14:15.600
Вот есть папка с новостями, которые я сам краулил.

01:14:15.800 --> 01:14:18.780
Да, это новости за январь и декабрь последние.

01:14:20.220 --> 01:14:21.740
Вот ссылку на эту папку я давал.

01:14:21.820 --> 01:14:23.380
Вы ее копируете себе на диск.

01:14:23.380 --> 01:14:25.240
Копи-то-драйв.

01:14:26.080 --> 01:14:33.980
А потом нам нужно к Google коллабу вот этой вот кнопкой

01:14:33.980 --> 01:14:38.440
или вот этим вот вызовом подмонтировать, добавить драйв.

01:14:38.540 --> 01:14:39.840
Вот здесь так.

01:14:40.060 --> 01:14:42.800
А, если я свой драйв покажу, да, то вы все увидите.

01:14:42.800 --> 01:14:46.520
В общем, теперь у нас в коллабе появляется файловая система,

01:14:46.700 --> 01:14:48.480
которая мапится на Google Drive.

01:14:48.720 --> 01:14:53.200
И на этом Google Drive должна быть скопирована папка с этими новостями.

01:14:54.340 --> 01:14:56.040
Сейчас будет долго ставиться.

01:14:56.240 --> 01:14:56.920
Да, пусть он...

01:14:56.920 --> 01:14:59.320
А, вот здесь есть ссылка на этот датасет.

01:15:00.880 --> 01:15:02.340
Хорошо, определяем секреты.

01:15:02.460 --> 01:15:04.400
Теперь в секретах нам нужны обе модели.

01:15:04.540 --> 01:15:07.800
Модель для генерации текста и модель для семантических векторов.

01:15:07.800 --> 01:15:13.800
Так, пока он ставится, я посмотрю на вопрос.

01:15:14.460 --> 01:15:15.260
Вопрос такой.

01:15:19.940 --> 01:15:22.560
Или никак.

01:15:23.560 --> 01:15:24.500
Хороший вопрос.

01:15:28.000 --> 01:15:34.000
Ну, я в системном промпте постоянно писал, на каком языке ей отвечать.

01:15:34.000 --> 01:15:40.800
Про то, на каком думать, я не писал.

01:15:41.860 --> 01:15:44.200
Я, наверное, не могу ответить на этот вопрос.

01:15:45.540 --> 01:15:46.700
Можно спросить DeepSync.

01:15:47.740 --> 01:15:48.580
Ну, там так.

01:15:48.660 --> 01:15:49.920
Оно или заработает, или нет.

01:15:50.340 --> 01:15:51.880
Возможно, на какой-то модели заработает,

01:15:52.000 --> 01:15:53.320
а на какой-то перестанет работать.

01:15:54.920 --> 01:15:55.000
Это...

01:15:55.660 --> 01:15:58.020
Возможно, эта инструкция только помешает.

01:15:58.840 --> 01:15:59.300
Не знаю.

01:15:59.300 --> 01:16:04.300
Так, мы ставим...

01:16:06.520 --> 01:16:12.380
Да, значит, код из первой лабы мы расширяем таким методом IndexFiles.

01:16:14.880 --> 01:16:15.760
Ага.

01:16:18.580 --> 01:16:19.740
Google Drive.

01:16:19.740 --> 01:16:28.320
Drive для десктопа нам не надо.

01:16:28.900 --> 01:16:32.740
После установки Sentence Transformer, оно требует перезапустить...

01:16:32.740 --> 01:16:39.160
Так, ну, все отлично.

01:16:39.340 --> 01:16:40.420
На демо все сломалось.

01:16:40.560 --> 01:16:41.660
Это то, что требовалось.

01:16:42.100 --> 01:16:44.600
Оно требует перезапустить сессию.

01:16:44.740 --> 01:16:45.960
Хорошо, пусть перезапускает.

01:16:46.140 --> 01:16:50.300
И для того, чтобы Sentence Transformers с адекватной скоростью работали,

01:16:50.380 --> 01:16:52.980
вот здесь у вас должен быть T4 runtime.

01:16:52.980 --> 01:16:58.780
Потому что CPU runtime прям совсем медленно работает.

01:16:59.340 --> 01:17:02.980
Вот, надо ставить T4 и должно повести.

01:17:03.100 --> 01:17:05.480
Он должен быть T4 runtime.

01:17:06.180 --> 01:17:08.560
И он должен заработать.

01:17:09.600 --> 01:17:11.900
Но иногда бывает, что не работает.

01:17:12.760 --> 01:17:14.940
Так, у нас немного минут осталось.

01:17:16.200 --> 01:17:17.020
Значит, смотрите.

01:17:17.020 --> 01:17:23.480
В качестве векторного хранилища, в качестве такой базы данных с семантикой

01:17:23.480 --> 01:17:25.180
мы используем хрому.

01:17:26.300 --> 01:17:27.620
Хрома – это...

01:17:27.620 --> 01:17:28.800
Здесь это библиотека.

01:17:30.220 --> 01:17:31.240
Здесь ее создаем.

01:17:31.620 --> 01:17:36.720
И она будет хранить наши файлы с семантическими векторами.

01:17:38.000 --> 01:17:40.900
Таким образом, мы подключаемся к хроме.

01:17:41.140 --> 01:17:43.040
Это самый минималистичный способ.

01:17:43.220 --> 01:17:46.720
Он настолько минималистичный, что файлы он сохранит.

01:17:47.020 --> 01:17:47.720
Занять не будет.

01:17:48.120 --> 01:17:51.220
Но поскольку у нас демонстрация, то это нормально.

01:17:51.460 --> 01:17:54.380
То есть пока работает это окружение Google Коллаба,

01:17:55.000 --> 01:17:56.200
хрома будет что-то помнить.

01:17:56.540 --> 01:17:57.520
А потом не будет.

01:17:57.880 --> 01:18:00.320
Чтобы файлы сохранялись, надо сделать persistent.

01:18:01.140 --> 01:18:05.480
Но непонятно, как persistent у вас будет оставаться на Google Коллабе.

01:18:05.980 --> 01:18:08.460
В общем, тоже с этим можно разобраться.

01:18:09.100 --> 01:18:10.280
Подключились к хроме.

01:18:10.760 --> 01:18:14.880
И теперь хрома она только хранит.

01:18:15.020 --> 01:18:16.500
Она не считает эти вектора.

01:18:16.500 --> 01:18:20.340
Надо ее параметризовать, передавая ей функцию эмбеддингов.

01:18:20.740 --> 01:18:21.740
Функция эмбеддингов.

01:18:22.080 --> 01:18:26.060
В данный момент мы сейчас будем использовать MiStral.

01:18:26.360 --> 01:18:30.480
MiStral и ее модель эмбеддингов, чтобы считать эти семантические вектора.

01:18:31.180 --> 01:18:34.320
Дальше мы индексируем файлы, передавая путь.

01:18:34.320 --> 01:18:40.300
Здесь в качестве пути мы пишем такой контент.

01:18:40.400 --> 01:18:40.700
Drive.

01:18:40.820 --> 01:18:45.960
Drive это подмонтированный Google Drive, который надо этой кнопкой было подмонтировать.

01:18:46.760 --> 01:18:49.340
Здесь вы указываете путь, куда вы ее присоединили.

01:18:49.440 --> 01:18:55.660
Эту папку, имя коллекции и небольшое количество параметров, которые объясню зачем нужны.

01:18:55.660 --> 01:19:03.940
Индекс файл просто пробегает по всем файлам по данному пути, создает коллекцию.

01:19:04.320 --> 01:19:08.440
При создании коллекции мы определяем ее имя и функцию для создания эмбеддингов,

01:19:08.580 --> 01:19:14.620
которая будет вызываться для индексации текстов в семантические вектора.

01:19:14.620 --> 01:19:19.440
Дальше мы читаем эти файлы и добавляем как тексты.

01:19:20.760 --> 01:19:22.780
Вот здесь мы добавляем их в коллекцию.

01:19:22.960 --> 01:19:26.920
То есть здесь просто напоминает классический NoSQL.

01:19:28.040 --> 01:19:32.760
Просто отправляем тексты на хранение, добавляем идентификаторы и метадату.

01:19:33.160 --> 01:19:37.900
Но, к сожалению, мы сейчас уже не успеваем поговорить про идентификаторы и метадату.

01:19:38.120 --> 01:19:40.780
Это очень NoSQL, как Mongo.

01:19:41.100 --> 01:19:41.540
Вот это все.

01:19:41.540 --> 01:19:43.800
Здесь еще есть Sleep.

01:19:43.940 --> 01:19:45.160
Sleep, смотрите, зачем надо.

01:19:45.260 --> 01:19:51.640
Mistral – это бесплатный сервис, и поэтому он работает только раз в секунду.

01:19:52.100 --> 01:19:55.560
Там у него есть ограничения по частоте вызовов.

01:19:55.800 --> 01:19:57.680
Он работает со скоростью раз в секунду.

01:19:58.200 --> 01:20:05.780
После того, как я проиндексировал переданные файлы, я задаю тестовый вопрос

01:20:05.780 --> 01:20:11.400
и смотрю первые три результата, которые Хрома выдаст.

01:20:11.540 --> 01:20:18.100
Так, я думаю, что у нас есть шансы увидеть результат индексации файлов.

01:20:22.100 --> 01:20:23.140
Так, он поехал.

01:20:23.320 --> 01:20:24.420
Может быть, нам повезет.

01:20:25.060 --> 01:20:29.340
Значит, здесь, в общем, в Хроме будет создан индекс.

01:20:29.420 --> 01:20:33.540
Он будет хранить семантические вектора и исходные тексты наших файлов.

01:20:33.540 --> 01:20:41.860
А вторая ячейка делает то же самое, но с использованием библиотеки Sentence Transformer.

01:20:41.960 --> 01:20:46.280
Это значит, что будет использоваться не API какой-то внешний,

01:20:46.320 --> 01:20:50.540
но вот на этом Python окружении, который Google Collab запустит,

01:20:52.620 --> 01:20:54.440
скачает библиотеку.

01:20:54.440 --> 01:20:55.560
Вот здесь остался output.

01:20:56.100 --> 01:21:01.340
Он скачивает библиотеку Sentence Transformer и скачивает модель.

01:21:01.440 --> 01:21:05.040
В качестве модели я использую китайскую модель BGE-M3.

01:21:05.860 --> 01:21:08.480
Она оказалась состоятельна с русским языком,

01:21:08.620 --> 01:21:12.180
а вот MPNet, которую я демонстрировал до этого, несостоятельна.

01:21:12.180 --> 01:21:16.000
И здесь можете первый раз это запустить на CPU,

01:21:16.240 --> 01:21:18.340
и оно будет отрабатывать минут 15.

01:21:20.400 --> 01:21:22.620
А чтобы отработало за полторы секунды,

01:21:22.700 --> 01:21:26.820
вот у вас должно здесь T4 вам должно выдать,

01:21:27.040 --> 01:21:30.620
и у него должно начать расти потребление GPU.

01:21:31.760 --> 01:21:35.000
Бывает, что T4 выделяется, но GPU не растет

01:21:35.000 --> 01:21:37.180
и считается на CPU 15 минут.

01:21:38.320 --> 01:21:40.160
Вот такие тут интересные эксперименты.

01:21:40.160 --> 01:21:44.900
И здесь еще, смотрите, но Sentence Transformer не нужен sleep,

01:21:45.120 --> 01:21:48.540
потому что он выполняется на окружении Google Collab.

01:21:49.320 --> 01:21:52.600
И ему интересно передавать большой batch size.

01:21:54.720 --> 01:21:57.760
Чем больше, тем быстрее он это все переживет.

01:21:58.800 --> 01:22:04.800
О, кстати, у нас пробежала индексация с помощью Mestral.

01:22:05.220 --> 01:22:07.400
А для Mestral нужен маленький batch size.

01:22:07.400 --> 01:22:11.360
Почему? Потому что там он ограничен по размеру реквеста.

01:22:11.620 --> 01:22:15.400
Если поставить batch size 10, то 10 текстов уже не влезут.

01:22:16.220 --> 01:22:17.440
Надо передавать по 8.

01:22:18.400 --> 01:22:18.880
Вот.

01:22:19.040 --> 01:22:21.640
И он, смотрите, проиндексировал новости.

01:22:21.780 --> 01:22:24.040
Там их что-то 30 новостей где-то так.

01:22:24.040 --> 01:22:28.900
И я спросил тестовый запрос про резервы центральных банков.

01:22:29.000 --> 01:22:30.680
И он мне выдал финансовые новости.

01:22:31.460 --> 01:22:35.120
Это, мне кажется, показывает, что семантический поиск работает.

01:22:36.080 --> 01:22:38.300
По крайней мере, с SMP.net он не работал.

01:22:38.300 --> 01:22:46.300
Теперь у меня есть коллекция в хроме, которая имеет семантический индекс по новостям.

01:22:46.960 --> 01:22:51.040
Попробуем запустить, наверное, для наглядности SMP.net Transformers.

01:22:52.180 --> 01:22:55.600
Сейчас, если GPU выдадут, то он быстро прибежит.

01:22:55.900 --> 01:22:58.980
Если GPU не будет, то будет очень долго.

01:22:58.980 --> 01:23:09.120
В принципе, выполняется та же самая операция, но теперь языковая модель запускается на Python окружении Google Collab.

01:23:12.420 --> 01:23:14.100
Дальше что происходит?

01:23:14.280 --> 01:23:20.720
Теперь мы в код первой лабораторной, он сабмит.

01:23:20.720 --> 01:23:38.140
В начало добавляем небольшой кусок кода, который из хрома извлекает текст по пользовательскому запросу и конкатинирует, вставляет эти найденные документы в контекст запроса пользователя.

01:23:39.200 --> 01:23:44.560
Вот здесь конкатинированные новости и предыдущую историю диалога тоже передает языковой модели.

01:23:46.080 --> 01:23:48.160
Так, GPU как-то не растет.

01:23:48.160 --> 01:23:50.800
А, вот сейчас происходит скачивать.

01:23:50.900 --> 01:24:00.020
Вот я хотел показать, как он с Hagenface, Stentons Transformers получает гигабайты весов модели BGE-M3.

01:24:01.180 --> 01:24:04.300
Так, ну, это может быть долго у нас займет.

01:24:05.120 --> 01:24:07.740
Смотрите, ну, в итоге мы получаем чат.

01:24:07.740 --> 01:24:17.120
Только теперь, когда я спрошу его про ситуацию на рынке продовольствия, он поищет новости про пшеницу.

01:24:17.120 --> 01:24:21.000
Ну, поищет про продовольствие и найдет про пшеницу.

01:24:23.000 --> 01:24:23.860
But not you, так.

01:24:23.980 --> 01:24:28.540
Он уже на нас жалуется, что мы получили GPU, но его не используем.

01:24:30.140 --> 01:24:33.180
Вот ему надо докачаться, и он начнет использовать...

01:24:33.180 --> 01:24:35.900
О, вот, смотрите, характерная картинка, да?

01:24:35.980 --> 01:24:46.080
Он начал потреблять GPU, и это значит, что он за полторы секунды отсчитает коллекцию с новостями.

01:24:46.080 --> 01:25:03.220
Ну, собственно, все, мы получим RAG, который будет для ответа на запросы выполнять поиск в нашей корпоративной базе данных из новостей экономики.

01:25:03.220 --> 01:25:13.980
Так, да, все, он построил индекс по новостям и сгенерировал ответ на запрос тестовый про финансовые новости.

01:25:14.220 --> 01:25:15.880
Он ответил финансовыми новостями.

01:25:15.880 --> 01:25:31.180
Теперь мы можем запустить наш RAG, и RAG начнет отвечать не расплывчатыми формулировками, а отвечать конкретно новостями.

01:25:31.180 --> 01:25:58.740
Так, вот здесь он тестовый вывод, он печатает новости, которые нашел.

01:25:58.740 --> 01:26:06.740
Теперь вот он про пшеницу и кофе передает в контекст и отвечает на вопрос про продовольствие, используя текст этих новостей.

01:26:11.480 --> 01:26:14.600
Значит, по лабораторной, по практической все.

01:26:14.740 --> 01:26:19.440
Мы построили RAG-систему, то, что требовалось от нас.

01:26:19.440 --> 01:26:31.440
И у меня остался заключительный раздел из шести слайдов про будущие этапы, да, про развитие, в какую сторону эти RAG-системы развиваются.

01:26:31.440 --> 01:26:37.240
Так, вопрос про batch size.

01:26:37.360 --> 01:26:39.380
Batch size – это просто количество файлов.

01:26:40.020 --> 01:26:42.800
Количество файлов, отправляемое за один раз.

01:26:43.340 --> 01:26:48.300
Ну, просто там, если по одному, то будет очень долго, поэтому вы можете все проверить.

01:26:48.420 --> 01:26:51.980
В этих вот системах с языковыми моделями там очень важно batching.

01:26:51.980 --> 01:26:54.380
Без batch'а там все очень плохо.

01:26:54.580 --> 01:26:57.280
По одному медленно поставил 10.

01:26:57.960 --> 01:27:00.220
Мистераль сказал, что он столько не будет обрабатывать.

01:27:01.840 --> 01:27:06.860
А вот 8 – влезает его контекст.

01:27:06.940 --> 01:27:12.380
Но он там выдает мистраль конкретные цифры, сколько ему прислали и сколько он собирается обрабатывать.

01:27:12.380 --> 01:27:19.380
Теперь смотрите, обязательная часть у нас как бы завершилась, да, потому что вот дополнительная не влезла.

01:27:19.760 --> 01:27:24.520
Я продолжаю вещать, запись будет, но уже вот как бы на ваше усмотрение.

01:27:24.640 --> 01:27:33.920
Я продолжаю, да, или здесь есть вопросы по практической части, потому что мы переходим к приложению, приложению Applications.

01:27:35.060 --> 01:27:35.540
Вопросы?

01:27:40.280 --> 01:27:41.240
Нет вопросов?

01:27:42.380 --> 01:27:46.220
Вот можете повторить еще раз?

01:27:46.760 --> 01:27:58.820
Когда мы считаем, собственно говоря, семантику, мы это считаем какой-то, опять же, какой-то библиотекой или, грубо говоря, их много, она одна, мы сами ее написали.

01:27:59.960 --> 01:28:02.120
Ага, как считается семантика?

01:28:03.360 --> 01:28:09.760
Смотрите, в начале индекс файлов, да, индекс файл создается коллекция.

01:28:09.760 --> 01:28:15.900
Мы в хроме объявляем, что у нас будет такая коллекция и передаем ей функцию, которая будет вычислять эмбеддинги.

01:28:15.900 --> 01:28:19.680
И здесь у меня используются два механизма.

01:28:19.860 --> 01:28:23.880
Вот сначала вот есть вот эмбеддинг API, и первый вызов.

01:28:24.000 --> 01:28:30.880
Я создаю Finance Embedding API коллекцию, передавая ей Embedding API, то есть это API мистраля.

01:28:30.880 --> 01:28:41.880
Теперь при индексации файлов вызывается мистраль для получения этих векторов, и запрос, который я отправляю, сначала векторизуется с помощью запроса к мистралю.

01:28:42.440 --> 01:28:46.760
Мистраль отвалится, а запрос ответить не удастся.

01:28:46.760 --> 01:28:56.940
А второй ход, как бы, альтернатива, да, я на этом Python окружении запускаю довольно тяжеловесную библиотеку Sentence Transformer.

01:28:57.260 --> 01:28:59.740
Вы видели, сколько она выкачивается, как она стартует.

01:29:00.460 --> 01:29:06.140
Запускаю ее для использования модели BGE M3, и вы видели, как она скачивается.

01:29:06.280 --> 01:29:12.840
Эти все гигабайты вот скачаются, устанавливаются файлами на этот Python узел.

01:29:12.840 --> 01:29:22.860
И теперь оно, используя графический ускоритель, который там есть, тендерный ускоритель, который есть на этом окружении, оно нам рассчитает.

01:29:24.820 --> 01:29:29.440
Вот используя эту функцию, теперь это Sentence Transformers Embedding функция.

01:29:29.520 --> 01:29:35.160
И теперь я передаю эту Embedding функцию создания хромоколлекции.

01:29:35.160 --> 01:29:49.660
И теперь хромоколлекция при индексации файлов и при ответе на запрос будет вызывать библиотеку Sentence Transformers, которую мы вызвали, передавая ей BGE M3.

01:29:50.740 --> 01:29:55.560
А почему именно, вот вопрос, почему именно ее, почему не другую, или она одна такая?

01:29:55.560 --> 01:30:10.700
Это на Hagen Face, хороший вопрос, на Hagen Face можно пойти, ну вот оно, да, вот Hagen Face, да, вот это со второй лабы, вот модель MPNet.

01:30:10.880 --> 01:30:18.940
Вот видите, оно, это модель на Hagen Face, да, оно уже не Text Generation, оно Sentence Similarity, эта задача называется.

01:30:18.940 --> 01:30:22.760
То есть эти модели, они под определенные задачи есть.

01:30:22.960 --> 01:30:27.720
И наша задача, это вот текст вектора, и эта задача называется Sentence Similarity.

01:30:29.000 --> 01:30:35.280
Модель MPNet, это, наверное, там вторая, третья модель, которую вам порекомендуют, если вы в Гугле будете искать там BERT модель.

01:30:35.280 --> 01:30:47.000
То есть, в принципе, их много, я могу попробовать с одной, я могу попробовать со второй, я могу попробовать с третьей, и вот в итоге, там, я не знаю, какая мне понравится, что и буду итоговый продукт делать, да?

01:30:47.000 --> 01:30:59.460
Да, я на второй лабе, да, я показывал, как MPNet считает осмысленные семантические вектора, она считает, но вот эти финансовые новости она мне плохо находила.

01:31:00.060 --> 01:31:04.280
То есть на вопрос про резервы центральных банков она не финансовые новости находила.

01:31:05.220 --> 01:31:16.420
И, кстати, что касается этой модели, можно прямо вот здесь вот зайти и попробовать, поставить сюда запрос, поставить сюда там три документа и посмотреть, как она их ранжирует.

01:31:16.420 --> 01:31:21.300
Вот у нас, сейчас, если вы дослушаете, у меня там будет еще про немного другие модели, мы увидим, как это работает.

01:31:22.000 --> 01:31:23.060
Молчу, спасибо.

01:31:24.140 --> 01:31:25.880
Нет, вопросы задавайте.

01:31:26.460 --> 01:31:28.280
Моделей имбеддингов много, действительно.

01:31:28.500 --> 01:31:33.400
Миша показал две разных модели и два разных способа их, собственно говоря, активации.

01:31:33.400 --> 01:31:36.800
Тут единственный нюанс, который, может быть, стоит еще дополнительно упомянуть.

01:31:36.800 --> 01:31:50.600
Если вы делаете вектор по своей документации и потом спрашиваете, то вы должны использовать одну и ту же модель имбеддингов, потому что между собой, естественно, эти вектора генерятся разными, разными моделями.

01:31:51.880 --> 01:31:52.700
Да, да, да.

01:31:53.700 --> 01:31:54.820
Эти нюансы, да.

01:31:56.180 --> 01:31:57.420
Так, еще вопросы?

01:31:57.420 --> 01:32:04.400
Нет, ну, если появятся, пишите.

01:32:05.300 --> 01:32:07.680
Будем рассматривать интересные вопросы.

01:32:08.220 --> 01:32:10.140
Значит, куда этот рак расширяется?

01:32:10.260 --> 01:32:11.320
По каким направлениям?

01:32:14.000 --> 01:32:17.380
По каким направлениям расширяется рак?

01:32:17.380 --> 01:32:20.420
Так, ага, что-то стало обновляться.

01:32:22.520 --> 01:32:22.940
Да.

01:32:24.700 --> 01:32:28.800
Вот, мы с вами видели сейчас, как работает B-encoder.

01:32:28.940 --> 01:32:31.400
B-encoder, он насчитывает вектора.

01:32:31.820 --> 01:32:35.740
И вот этим вот сильна эта модель, что можно вектора насчитать, отложить.

01:32:36.180 --> 01:32:41.080
Потом приходит нам запрос, мы посчитали тоже вектор этой же модели, и они совместимы.

01:32:41.200 --> 01:32:43.580
Их можно по дистанции ранжировать.

01:32:43.580 --> 01:32:46.360
Это то, что нам нужно для семантического поиска.

01:32:48.020 --> 01:32:49.800
Но есть другой подход.

01:32:49.960 --> 01:32:51.480
Подход называется кросс-encoder.

01:32:52.140 --> 01:32:57.820
Он работает, вот это примерно тот же самый движок, который называется BERT, да.

01:32:58.240 --> 01:33:00.680
Но здесь еще добавлен классификатор.

01:33:01.080 --> 01:33:06.400
Классификатор, он эти вектора, да, он пересчитывает их в число, грубо говоря, от нуля до единицы.

01:33:06.620 --> 01:33:07.280
Похоже, не похоже.

01:33:08.360 --> 01:33:12.700
Но вот картинки похожи, да, но вот они по-разному устроены.

01:33:12.700 --> 01:33:16.740
Здесь, видите, запросы документа, они сходятся в одну строчку.

01:33:16.740 --> 01:33:18.420
Они конкатинируются, да.

01:33:19.140 --> 01:33:20.280
И классифицируются.

01:33:22.520 --> 01:33:29.500
Это используется, вот такая архитектура, которая справа, она используется для более точного ранжирования.

01:33:30.620 --> 01:33:34.460
То есть мы вот слева в B-encoder мы можем ранжировать, да.

01:33:34.460 --> 01:33:38.980
Но это будет не так точно, как с кросс-энкодером.

01:33:40.320 --> 01:33:43.880
И у меня, по-моему, да, есть пример.

01:33:45.220 --> 01:33:46.180
Классный пример.

01:33:47.780 --> 01:33:48.820
Пример классный.

01:33:49.780 --> 01:33:50.000
Да.

01:33:50.000 --> 01:33:56.400
Нет, так я хотел открыть, хотел вот так открыть.

01:33:57.780 --> 01:33:58.380
Да.

01:33:59.480 --> 01:34:01.880
Как работает кросс-энкодер?

01:34:03.140 --> 01:34:07.020
Поступил такой вопрос из поискового сообщества.

01:34:07.020 --> 01:34:12.620
Пользователь вводит Милан, но выяснилось, что Миланов, они есть почти в каждой стране.

01:34:14.040 --> 01:34:14.240
Вот.

01:34:14.340 --> 01:34:22.960
И давайте посмотрим, как кросс-энкодер разрешит неопределенность с тем, из какой страны был этот Милан.

01:34:22.960 --> 01:34:37.960
Вот здесь я вызываю, значит, задача, то есть кросс-энкодер решает уже задачу, которая называется Zero Short Classification.

01:34:39.940 --> 01:34:48.960
Передаем Милан сюда, в эту демку, и через запятую возможные классы, к какому из них классифицировать.

01:34:48.960 --> 01:34:54.120
И, в общем, когда загрузится, когда загрузится, он нам классное покажет.

01:34:57.120 --> 01:34:59.960
А здесь мы можем...

01:35:01.980 --> 01:35:03.340
Это кросс-энкодер.

01:35:05.740 --> 01:35:06.540
Вот.

01:35:06.820 --> 01:35:12.960
Так, сейчас я очень быстро сделаю демку на MPNet.

01:35:12.960 --> 01:35:18.960
Пока у нас загружается кросс-энкодер.

01:35:22.960 --> 01:35:24.080
Так.

01:35:34.960 --> 01:35:36.560
Так.

01:35:36.780 --> 01:35:40.720
Вот так.

01:35:42.960 --> 01:35:48.900
Итак, у нас есть канадский Милан, итальянский и американский, да?

01:35:48.980 --> 01:35:49.640
Какой же победит?

01:35:49.720 --> 01:35:51.440
Какой же из них будет ближе?

01:35:51.540 --> 01:35:53.140
Нет, ой, нам больше не надо сэнкодер.

01:35:53.220 --> 01:35:53.700
Не знаю, как.

01:35:55.980 --> 01:35:58.100
Но они тоже все холодные, не грузятся.

01:35:58.300 --> 01:35:58.480
Так.

01:35:58.660 --> 01:36:03.060
У нас был такой же запрос задан кросс-энкодеру.

01:36:04.240 --> 01:36:05.380
Нет, он все считает.

01:36:05.380 --> 01:36:09.200
Да, не удалось.

01:36:09.700 --> 01:36:11.140
Не удалось показать демку.

01:36:13.140 --> 01:36:14.200
Так, грохнем.

01:36:15.880 --> 01:36:17.000
Может, здесь покажется?

01:36:18.060 --> 01:36:19.160
Здесь не показывается.

01:36:20.740 --> 01:36:22.360
Да, да, не удалось.

01:36:22.720 --> 01:36:24.760
Надо картинки сохранять.

01:36:24.760 --> 01:36:31.240
Покажу.

01:36:31.780 --> 01:36:32.120
Ладно.

01:36:32.280 --> 01:36:36.360
Ну, в общем, значит, это выносится на домашнее упражнение.

01:36:36.480 --> 01:36:41.300
Вот в слайдах, в комментариях есть, значит, значения, с которыми вызывать ссылки на демки.

01:36:41.300 --> 01:36:48.300
И вы посмотрите, как HagenFace в рантайме, когда загрузит модели, ответит на вот эти два запроса.

01:36:49.680 --> 01:36:54.300
То есть мы одну и ту же задачу двумя разными моделями решим.

01:36:55.100 --> 01:36:58.300
И там будет видно, какая из моделей ее лучше решает.

01:36:58.900 --> 01:37:00.300
Вот такая, значит, домашняя работа.

01:37:03.060 --> 01:37:07.160
Ну, не могу я никак это разогнать.

01:37:07.160 --> 01:37:13.620
А, сейчас, может, горячий сезон, что Америка проснулась, начинает пыркать в этот HagenFace, и все очень долго.

01:37:14.740 --> 01:37:17.440
Да, не продумал, надо было скриншоты сохранить.

01:37:17.660 --> 01:37:17.880
Ладно.

01:37:18.220 --> 01:37:25.760
В общем, переходим к следующему, да, развитию моделей рагов.

01:37:26.380 --> 01:37:31.680
С чем вы столкнетесь, да, первое, с чем мы сталкиваемся, когда делаем этот раг,

01:37:31.680 --> 01:37:38.120
что пользователь приходит на демо и вводит какой-то короткий запрос, например, Nike, конкретное слово.

01:37:38.580 --> 01:37:48.160
И вот эти модели семантические, да, они отдельные короткие слова, термины или какие-то редкие там профильные термины, да, они их не ловят.

01:37:49.380 --> 01:37:53.840
Семантику выдают плохую, и семантический поиск не всегда хорошо работает.

01:37:53.840 --> 01:38:00.860
А пользователь на каком-то демо, да, то есть это может быть, вот, то есть может быть для бизнеса, да,

01:38:01.280 --> 01:38:04.840
поиск по семантике, он бы в общем для всех запросов, он бы хорошо отработал,

01:38:05.560 --> 01:38:12.560
но на демо надо найти конкретный бренд, да, который нужен, любимый бренд начальника, условно говоря.

01:38:13.520 --> 01:38:18.700
А система с семантическими векторами, вот эта вот слева, которая, по векторам, она его может не найти.

01:38:18.700 --> 01:38:23.760
В общем, тут ничего удивительного не будет, это довольно предсказуемо.

01:38:24.440 --> 01:38:31.480
И поэтому справа возводится система, которая ищет по совпадению слов, да, текстового поиска,

01:38:31.600 --> 01:38:37.900
система текстового поиска, и потом мы сливаем результаты текстового и семантического поиска.

01:38:38.420 --> 01:38:43.660
Вот, это называется система гибридного поиска, и если мы говорим о RAC-системе,

01:38:44.100 --> 01:38:46.460
там должен быть гибридный поиск, без этого никак.

01:38:46.460 --> 01:38:53.480
Так, все это дело осложняется тем, что, ну, никак адекватно, прямо скажем,

01:38:53.600 --> 01:38:59.700
адекватно нельзя слить результаты с разных моделей, то есть это математически эту задачу даже поставить невозможно.

01:39:00.500 --> 01:39:06.780
Ну, это как, здесь слева у нас столеная, да, справа горячая, вот, а надо перемешать результаты.

01:39:07.120 --> 01:39:11.540
То есть мы их как-то можем перемешать, но конкретно эта задача не решена.

01:39:11.540 --> 01:39:17.340
Дальше, когда вы начинаете заниматься RAC-ом, вы найдете очень много информации про то, как резать чанки.

01:39:17.540 --> 01:39:20.700
И, в общем, много блогов исписано про то, как резать чанки.

01:39:22.280 --> 01:39:29.540
Этот слайд посвящен вот последнему слову в этой теме, он называется «Поздний чанкинг».

01:39:29.540 --> 01:39:32.140
Очень сложно объяснить, что это.

01:39:32.780 --> 01:39:37.520
В общем, я вас призываю к тому, чтобы не зацикливаться на чанкинге.

01:39:37.900 --> 01:39:43.340
Вам, скорее всего, он не нужен, потому что эти все исследования были написаны для старых моделей,

01:39:43.400 --> 01:39:45.060
у которых контекст был короткий.

01:39:45.620 --> 01:39:51.360
Сходите в карточку модели и посмотрите, какого размера контекст ваша модель обрабатывает.

01:39:51.360 --> 01:39:55.860
Он уже, скорее всего, большой, и чанкать уже ничего не надо.

01:39:56.080 --> 01:40:00.800
То есть поздний чанкинг мы изучать не будем.

01:40:01.600 --> 01:40:05.160
Просто учитывайте, что чанкинг — это не так важно.

01:40:05.300 --> 01:40:07.580
Уже все, скорее всего, влезет в контекст модели.

01:40:09.980 --> 01:40:16.980
Так, векторное хранилище — это не какая-то транзиентная библиотека хрома,

01:40:17.120 --> 01:40:19.980
которую я запустил так, что она даже файлов не оставляет.

01:40:19.980 --> 01:40:23.420
Это вполне себе, можно сказать, векторные базы данных,

01:40:23.660 --> 01:40:27.380
которые там уже администрируются отдельными инженерами.

01:40:27.860 --> 01:40:34.340
Здесь перечислены наиболее популярные и распространенные векторные базы.

01:40:34.920 --> 01:40:36.420
Их функциональность мы рассмотрели.

01:40:36.540 --> 01:40:40.200
У них там много каких еще функций они предоставляют.

01:40:41.120 --> 01:40:43.960
Тот же гибридный поиск у них у всех есть.

01:40:43.960 --> 01:40:53.740
А в конце перечислены две системы предыдущих архитектур предыдущего поколения,

01:40:53.900 --> 01:40:57.660
которые активно развиваются и добавляются плагинами,

01:40:58.460 --> 01:41:01.960
расширяют свою функциональность в направлении векторного поиска.

01:41:01.960 --> 01:41:08.960
То есть классические системы тоже начинают...

01:41:09.840 --> 01:41:11.960
Классические системы, как Postgres и Elasticsearch,

01:41:12.820 --> 01:41:16.820
тоже умеют искать по векторам и активно в этом направлении развиваются,

01:41:16.960 --> 01:41:20.200
чтобы не отстать от рынка.

01:41:20.200 --> 01:41:25.560
А тот RAG, который мы с вами рассмотрели,

01:41:25.660 --> 01:41:28.180
выглядит вот примерно по такой схеме.

01:41:28.420 --> 01:41:30.580
У нас есть документы, которые мы проиндексировали.

01:41:30.680 --> 01:41:32.280
У нас есть пользовательский запрос,

01:41:32.820 --> 01:41:36.580
которые поступают в поиск, наш векторный движок поиска.

01:41:37.360 --> 01:41:40.860
Результат поиска отправляется в языковую модель.

01:41:40.940 --> 01:41:43.060
И она тут снежинкой обозначена для того,

01:41:43.140 --> 01:41:44.900
чтобы подчеркнуть то, что она не меняется.

01:41:44.900 --> 01:41:47.660
Она замороженная LLM.

01:41:47.840 --> 01:41:49.780
И это называется наивный рак.

01:41:49.780 --> 01:41:51.920
Мы с вами посмотрели на наивный рак,

01:41:52.080 --> 01:41:56.580
с чем приходится работать на практике.

01:41:57.160 --> 01:42:00.340
С тем, что он усложняется вот в этих аспектах.

01:42:02.060 --> 01:42:03.760
Вы, наверное, можете столкнуться с тем,

01:42:03.860 --> 01:42:06.060
что пользователь отправит очень короткий запрос.

01:42:06.500 --> 01:42:08.420
Этот короткий запрос надо будет расширять.

01:42:08.580 --> 01:42:12.920
То есть очень часто даже перед поиском в векторном хранилище

01:42:12.920 --> 01:42:16.860
мы с помощью языковой модели расширяем запрос.

01:42:17.020 --> 01:42:19.760
То есть вызываем отдельную языковую модель предварительно,

01:42:19.780 --> 01:42:23.880
и говорим, пользователь вот в этом диалоге,

01:42:24.300 --> 01:42:25.920
передаем предыдущие сообщения,

01:42:26.240 --> 01:42:29.140
спросил у нас вот этот короткий запрос.

01:42:30.520 --> 01:42:32.840
Особенно проблема с местоимениями.

01:42:33.120 --> 01:42:37.840
То есть пользователь ссылается на предыдущие предложения диалога.

01:42:38.840 --> 01:42:41.360
Нам их надо как-то расширить, этого смысла.

01:42:41.360 --> 01:42:46.820
И можем попросить языковую модель сгенерируя ключевые слова для векторного поиска.

01:42:47.340 --> 01:42:49.180
Это прям необходимо делать.

01:42:49.280 --> 01:42:52.360
Вот всякие предварительные преобразования запроса

01:42:52.360 --> 01:42:57.980
перед вызовом движка векторного поиска.

01:42:57.980 --> 01:43:05.680
Дальше результаты мы получили, и эти результаты иногда просто получаем слишком большие.

01:43:05.900 --> 01:43:08.420
Их надо как-то суммаризовать, уменьшить.

01:43:09.020 --> 01:43:12.380
И мы тоже можем несколько раз здесь вызывать языковую модель.

01:43:12.500 --> 01:43:18.980
Например, каждый из результатов попросить контекстуализировать для данного запроса.

01:43:18.980 --> 01:43:25.700
Этап переранжирования я вам показывал с кросс-энкодером.

01:43:26.260 --> 01:43:31.100
То есть выполняется более точное переранжирование с помощью кросс-энкодера.

01:43:31.200 --> 01:43:32.260
Это называется реранкер.

01:43:32.880 --> 01:43:39.300
И потом уже уточненный контекст поступает на генерацию в генерирующую языковую модель.

01:43:41.360 --> 01:43:44.680
Так, и последнее слово это графы в Ирак.

01:43:44.680 --> 01:43:48.840
Графы в Ирак – более сложная архитектура.

01:43:48.980 --> 01:43:51.880
Смотрите, до этого мы RIG, мы работали с текстами.

01:43:52.400 --> 01:43:56.840
Тексты везде, ищем релевантные тексты, из них строим контексты,

01:43:56.880 --> 01:43:58.740
отправляем на генерацию текстов.

01:43:59.280 --> 01:44:00.580
Что предлагает граф Ирак?

01:44:00.680 --> 01:44:04.040
Он предлагает выключить из текста структуру.

01:44:04.240 --> 01:44:07.500
То есть во время предварительной обработки документов,

01:44:07.500 --> 01:44:09.780
много раз вызывая языковую модель,

01:44:09.900 --> 01:44:16.900
попросить вычленить конкретные сущности из наших сырых текстов,

01:44:16.900 --> 01:44:20.920
их идентифицировать, проставить на них идентификаторы, построить связи.

01:44:21.460 --> 01:44:26.840
То есть на каждый текстовый документ задаются запросы в языковую систему,

01:44:26.980 --> 01:44:31.560
что какие здесь сущности в этом тексте, а какие между ними связи.

01:44:32.120 --> 01:44:36.500
Потом из этого строится такой граф, явный, я явного видна граф,

01:44:36.500 --> 01:44:43.640
который потом используется для ответа на вопрос следующим образом.

01:44:44.140 --> 01:44:48.100
Что мы в этом графе ищем вершины, про которые пользователь спрашивал,

01:44:48.580 --> 01:44:54.540
и добавляем в контекст окружающие вершины и отношения этих вершин.

01:44:55.060 --> 01:44:59.940
Отсюда генерируется более точный контекст и тоже отправляется на генерацию.

01:44:59.940 --> 01:45:05.880
То есть граф и враг — это попытка структурированием текста уйти от неточности,

01:45:06.000 --> 01:45:13.280
более точно подготовить ответ на вопрос пользователя.

01:45:14.200 --> 01:45:18.660
На практике он приводит к тому, что очень дорого вызывать языковые модели,

01:45:18.780 --> 01:45:23.380
их надо очень много раз вызывать, но такая многообещающая архитектура.

01:45:23.380 --> 01:45:28.780
И, по-моему, у меня на этом все.

01:45:29.440 --> 01:45:32.520
За сегодняшний воркшоп, который шел уже два часа,

01:45:34.340 --> 01:45:35.400
вызвали языковую модель.

01:45:36.440 --> 01:45:40.320
Самым простым образом создали для нее пользовательский интерфейс

01:45:40.320 --> 01:45:41.360
и программный интерфейс.

01:45:41.480 --> 01:45:43.900
Посмотрели, какие бывают модели, как их выбирают,

01:45:44.260 --> 01:45:46.720
как они запускаются на inference-серверах.

01:45:47.900 --> 01:45:51.200
Посмотрели, где лежат модели, как их искать и различать.

01:45:51.200 --> 01:45:55.440
Познакомились также с кодирующими и классифицирующими языковыми моделями.

01:45:56.200 --> 01:45:58.160
Помимо ранее виденных генерирующих,

01:45:58.680 --> 01:46:03.180
увидели, как кодирующие модели используются в векторных движках поиска

01:46:03.180 --> 01:46:09.660
и использовали их как компонент генерирующей системы RAG.

01:46:10.660 --> 01:46:20.220
У меня по контенту все, может быть, еще проснулись модели кросс-энкодера.

01:46:21.200 --> 01:46:23.640
Можно посмотреть красивую демку.

01:46:24.080 --> 01:46:24.280
Вот.

01:46:25.320 --> 01:46:25.660
Так.

01:46:26.160 --> 01:46:29.620
Кросс-энкодер, смотрите, насколько он категоричен.

01:46:30.200 --> 01:46:34.280
Он для итальянского Милана поставил оценку почти единица,

01:46:34.620 --> 01:46:38.200
а канадский и американский явно дискриминировал.

01:46:40.200 --> 01:46:40.440
Вот.

01:46:40.440 --> 01:46:42.080
Так отработал кросс-энкодер.

01:46:42.080 --> 01:46:47.100
Может быть, нам повезло и проснулся этот друг.

01:46:49.320 --> 01:46:49.600
Так.

01:46:50.800 --> 01:46:53.820
Ну, тут, наверное, придется заново это все пройти.

01:46:54.200 --> 01:46:54.560
Квест.

01:46:54.560 --> 01:47:03.640
Так.

01:47:05.180 --> 01:47:06.420
Вопросы какие-то есть?

01:47:06.520 --> 01:47:10.440
Мы уже вышли за все, что можно было.

01:47:10.440 --> 01:47:10.820
Так.

01:47:11.920 --> 01:47:12.560
Итальянский.

01:47:13.020 --> 01:47:14.780
Просто демка красивая, мне кажется.

01:47:16.140 --> 01:47:17.740
Вопросы, предложения, пожелания.

01:47:19.860 --> 01:47:20.100
Так.

01:47:20.220 --> 01:47:20.780
Канадский.

01:47:20.780 --> 01:47:28.120
Да, честно говоря, мучает вопрос про то, как обучаются

01:47:28.120 --> 01:47:29.540
GPT-модели.

01:47:31.240 --> 01:47:35.420
То есть, есть ли какой-то при обучении какой-то критерий

01:47:35.420 --> 01:47:37.120
выживаемости?

01:47:37.120 --> 01:47:40.400
Например, при генетических логаритмах мы поколение

01:47:40.400 --> 01:47:43.660
какое-то выращиваем, мы пытаемся их обучить чему-то.

01:47:43.940 --> 01:47:47.520
У нас есть критерии успешности, неуспешных мы отсеиваем.

01:47:47.520 --> 01:47:51.960
И дальше следующее поколение, эта копия там успешная.

01:47:52.420 --> 01:47:54.860
А вот GPT-модель, ну вот она тренируется на большом

01:47:54.860 --> 01:47:55.780
количестве текстов.

01:47:56.240 --> 01:47:58.380
Как она успешно тренировалась или неуспешно?

01:47:58.540 --> 01:47:59.320
Как это выясняется?

01:47:59.440 --> 01:48:01.240
Такими заданиями тоже выясняется?

01:48:01.440 --> 01:48:02.920
Или они предподготовлены?

01:48:03.020 --> 01:48:03.280
Или как?

01:48:05.940 --> 01:48:06.500
Так.

01:48:06.720 --> 01:48:07.780
Сейчас скажу.

01:48:08.820 --> 01:48:11.740
Ну, в конце у них есть оценка.

01:48:11.740 --> 01:48:14.020
А, ну, да, я понял.

01:48:14.560 --> 01:48:19.240
Это самый базовый алгоритм машинного обучения, вот на самом

01:48:19.240 --> 01:48:20.420
верхнем абстрактном уровне.

01:48:20.860 --> 01:48:22.680
У нас есть датасет.

01:48:23.940 --> 01:48:28.420
Датасет – это параметр ожидаемое значение.

01:48:28.680 --> 01:48:29.860
Параметр ожидаемое значение.

01:48:30.320 --> 01:48:33.160
Он просто бьется, ну, самое простое, да, разобьем на

01:48:33.160 --> 01:48:33.680
две части.

01:48:33.680 --> 01:48:37.980
Первую часть тестового заталкиваем в языковую

01:48:37.980 --> 01:48:38.300
модель.

01:48:38.480 --> 01:48:41.480
Она на них обучается, выучивает пары параметров

01:48:41.480 --> 01:48:42.000
значения.

01:48:43.140 --> 01:48:47.040
И потом мы можем ей передать те параметры, которые

01:48:47.040 --> 01:48:50.740
она не видела, посмотреть, насколько они далеко отстоят

01:48:50.740 --> 01:48:55.000
от ожидаемых результатов.

01:48:56.180 --> 01:48:58.060
Ну, вот на базовом уровне так происходит.

01:48:58.760 --> 01:49:02.660
Вторая часть – это уже, то есть на две части, да,

01:49:02.660 --> 01:49:05.240
первая часть – это мы для обучения используем, разбиваем

01:49:05.240 --> 01:49:07.340
датасет на две части, а вторая часть – для чего?

01:49:08.860 --> 01:49:09.380
Контроля.

01:49:11.780 --> 01:49:15.240
Но, смотрите, любой учебник по ML вы откроете, там вот

01:49:15.240 --> 01:49:18.380
даже вот этот примитивный пример, он уже сложнее,

01:49:18.480 --> 01:49:20.740
там три, три разбиения используются.

01:49:21.860 --> 01:49:26.160
Три, потому что третий, как бы, еще один элемент

01:49:26.160 --> 01:49:30.560
контроля, потому что вот с первыми двумя частями,

01:49:30.780 --> 01:49:32.600
да, вы пока делаете, у вас все равно может

01:49:32.600 --> 01:49:38.900
создаться феномен переобучения, что вы пока подбираете

01:49:38.900 --> 01:49:42.920
гиперпараметры, да, вот про валидацию выборку

01:49:42.920 --> 01:49:47.180
подсказывает, что она может переобучиться и привыкнуть

01:49:47.180 --> 01:49:49.880
ко второму вот этому датасету тестовому.

01:49:49.880 --> 01:49:56.460
поэтому есть еще один тест на тест, что она, ну, готова

01:49:56.460 --> 01:49:59.140
к третьему набору данных, которые вообще никогда не

01:49:59.140 --> 01:49:59.460
видела.

01:49:59.460 --> 01:50:05.220
это так, ну, следующий этап – это кросс-валидация, там

01:50:05.220 --> 01:50:10.640
она много таких разбиений, с учетом, что, ну, не переносятся

01:50:10.640 --> 01:50:11.720
оценки между ними.

01:50:11.980 --> 01:50:15.200
Ну, в общем, вот что-что, а вот это очень сильно проработано

01:50:15.200 --> 01:50:20.860
и прям в книжках про ML про это прям много что есть.

01:50:20.860 --> 01:50:25.280
И когда мы ее обучаем, мы ее обучаем на фиксированном

01:50:25.280 --> 01:50:28.900
все равно датасете, да, и говорим, наверное, что

01:50:28.900 --> 01:50:32.040
раз она обучилась на нем, значит, она может и на

01:50:32.040 --> 01:50:35.000
другие вопросы отвечать, да, то есть он же все равно

01:50:35.000 --> 01:50:36.580
какой-то конечный, правильно?

01:50:36.580 --> 01:50:40.600
Ну, мы идем и задаем ML какие-то вопросы, там, про

01:50:40.600 --> 01:50:46.600
программирование, и она отвечает, но ведь не факт же, что в

01:50:46.600 --> 01:50:48.520
контрольном датасете эти примеры были, да, то есть

01:50:48.520 --> 01:50:52.420
мы как-то все-таки экстраполируем эту валидацию и говорим, что

01:50:52.420 --> 01:50:53.280
она все-таки работает.

01:50:53.700 --> 01:50:57.840
Ну, смотрите, крупные модели, они практически на огромном

01:50:57.840 --> 01:51:00.280
датасете обучены, то есть считайте, что туда

01:51:00.280 --> 01:51:04.080
пожали весь интернет, да, по большому счету, если

01:51:04.080 --> 01:51:08.360
мы берем топовые модели, и поэтому в целом, да, там

01:51:08.360 --> 01:51:12.300
у них точность ответа высокая, и область знания, она широкая

01:51:12.300 --> 01:51:12.580
очень.

01:51:14.400 --> 01:51:15.080
Окей, спасибо.

01:51:15.720 --> 01:51:20.520
А вот эти модели, публичные, там, типа того же самого

01:51:20.520 --> 01:51:25.140
ChatGPT, они на наших запросах дообучаются на лету как-то?

01:51:25.620 --> 01:51:27.020
Или это только статика?

01:51:27.560 --> 01:51:28.120
Статика.

01:51:30.720 --> 01:51:33.660
Эти запросы, которые ты подаешь, они могут откладываться

01:51:33.660 --> 01:51:36.460
в сторону для обучения GPT-5 гипотетически.

01:51:36.580 --> 01:51:38.940
Именно поэтому мы все обеспокоены тем, чтобы туда

01:51:38.940 --> 01:51:39.860
утечек не было.

01:51:41.820 --> 01:51:43.880
Да, я как раз немножко правильно прочитал.

01:51:43.880 --> 01:51:48.820
Да, Андрей, они заявляют, что если ты там сделаешь

01:51:48.820 --> 01:51:51.500
соответствующие настройки, там есть галочка, да, что

01:51:51.500 --> 01:51:57.100
не хранить, никуда не передавать, вроде как они на них обучать

01:51:57.100 --> 01:51:59.320
не будут, да.

01:51:59.320 --> 01:52:02.520
Но при этом мы понимаем, что все равно запрос сохранится,

01:52:02.660 --> 01:52:06.980
и он где-то будет лежать, и если нужно будет его найти.

01:52:06.980 --> 01:52:09.540
Они будут обучаться на другой копии, не на той, которая

01:52:09.540 --> 01:52:10.600
до них приходит.

01:52:10.620 --> 01:52:13.100
Мне кажется, наоборот, интерес будет проявлять таким

01:52:13.100 --> 01:52:13.800
флажком.

01:52:15.880 --> 01:52:16.700
Возможно, да.

01:52:16.700 --> 01:52:18.640
Два пункта.

01:52:19.060 --> 01:52:20.100
Перед этим еще один.

01:52:20.180 --> 01:52:23.160
Во-первых, у нас досчитался B-encoder.

01:52:23.360 --> 01:52:26.500
B-encoder, смотрите, это то, как происходит поиск по

01:52:26.500 --> 01:52:30.160
семантическим векторам, и он показал так.

01:52:30.860 --> 01:52:32.700
В общем, он показывает.

01:52:36.900 --> 01:52:38.940
Милан не настолько итальянский.

01:52:38.940 --> 01:52:45.700
Но он все равно отдает предпочтение, нет, а вот, он отдает предпочтение

01:52:45.700 --> 01:52:50.120
итальянскому Милану, но разница не такая большая, да, то есть

01:52:50.120 --> 01:52:52.940
американский и канадский, он тоже вполне себе Милан.

01:52:53.480 --> 01:52:59.000
И сравним это с разницей, которую мы видим у кросс-энкодера.

01:52:59.400 --> 01:53:02.600
Видите, однозначно итальянский Милан.

01:53:02.600 --> 01:53:08.600
Вот это отличие, да, это почему в качестве ранкера используется

01:53:09.300 --> 01:53:12.180
вот эта вот вторая фаза поиска.

01:53:12.360 --> 01:53:16.580
У нас предварительно кандидаты отбираются B-encoder, да,

01:53:16.700 --> 01:53:18.600
а потом уже точные результаты мы можем переранжировать

01:53:19.200 --> 01:53:21.760
и прям точный ответ дать с помощью кросс-энкодера.

01:53:22.220 --> 01:53:25.760
Но кросс-энкодер медленнее, потому что он каждую пару вопрос-ответ

01:53:25.760 --> 01:53:27.260
генерирует заново.

01:53:28.840 --> 01:53:31.740
Так, был вопрос про память, да.

01:53:31.740 --> 01:53:36.740
Вот я подчеркивал, да, во всех лабах, да, о том,

01:53:36.820 --> 01:53:39.860
что это стейтлесс-система, и она как бы ничего не запоминает.

01:53:39.960 --> 01:53:43.140
И поэтому нам надо вот эту цепочку сообщений ей пересылать.

01:53:43.840 --> 01:53:46.600
И как бы надо понимать, что ее когда тренировали,

01:53:46.700 --> 01:53:49.580
она тоже уже много таких диалогов выполнила

01:53:49.580 --> 01:53:53.580
и много раз такие вот именно диалоги отвечала.

01:53:54.440 --> 01:53:56.360
Поэтому оно работает с диалогами.

01:53:56.600 --> 01:53:59.700
Единственное исключение, я про него упоминал, это MemGPT.

01:53:59.700 --> 01:54:03.840
Те библиотеки, есть много библиотек, можно найти,

01:54:04.020 --> 01:54:08.420
которые в общем называются MemGPT, и это, насколько я помню,

01:54:08.460 --> 01:54:11.580
в чат ГПТ появляется иногда такая плашка Memory Updated.

01:54:12.340 --> 01:54:13.060
Что она делает?

01:54:13.220 --> 01:54:17.220
Она истории этих диалогов, которые мы с ней ведем под нашу учетку,

01:54:17.260 --> 01:54:18.740
если мы это не запрещаем.

01:54:19.340 --> 01:54:22.560
Она их все записывает, сохраняет,

01:54:22.560 --> 01:54:26.220
но поскольку их может много накопиться, что она превысит контекст,

01:54:26.380 --> 01:54:32.460
она каждый раз при вызове делает раг по истории предыдущих диалогов.

01:54:32.700 --> 01:54:34.800
То есть мы с ней про живопись много раз говорили,

01:54:35.320 --> 01:54:38.700
у нее есть весь набор наших с ней предыдущих диалогов про живопись.

01:54:39.220 --> 01:54:42.280
В следующий раз мы что-то спрашиваем, про велосипеды мы у нее спросим.

01:54:42.280 --> 01:54:47.180
Она сделает раг по этой накопленной истории,

01:54:47.300 --> 01:54:50.060
оттуда что-нибудь достанет про картины велосипедистов

01:54:50.060 --> 01:54:55.180
и начнет нам отвечать про живопись на велосипеде и так далее,

01:54:55.600 --> 01:54:57.200
потому что Memory Updated.

01:54:58.540 --> 01:55:02.160
Но это вот дополнительный раг, который там есть.

01:55:06.020 --> 01:55:08.440
Но ядро, оно stateless.

01:55:08.440 --> 01:55:13.220
Вот у меня такой ответ на последний вопрос.

01:55:14.460 --> 01:55:19.540
А вот про раг, как оно понимает, где нужно остановиться?

01:55:20.560 --> 01:55:23.660
Это меня Саша Сильверстов навел на этот вопрос,

01:55:23.760 --> 01:55:26.900
когда был слайд еще на, по-моему, первой лабе,

01:55:27.020 --> 01:55:32.200
что определяется наиболее возможное следующее слово

01:55:32.200 --> 01:55:35.000
и генерируется в ответ, и мы в личке с ним обсуждаем.

01:55:35.400 --> 01:55:36.860
Спросил, а как же оно понимает,

01:55:36.860 --> 01:55:39.680
когда вот это вот наиболее последнее вероятное слово пропадает,

01:55:39.740 --> 01:55:40.620
и надо остановиться?

01:55:41.220 --> 01:55:42.900
А и в раге тоже самый вопрос.

01:55:43.080 --> 01:55:46.040
То есть как оно понимает, что набрало достаточно контекста

01:55:46.040 --> 01:55:49.100
или наоборот не набрало слишком мало контекста из рага?

01:55:50.020 --> 01:55:52.620
Где этот трэш-холд, какие-то отсечки, как это работает?

01:55:53.240 --> 01:55:54.380
Так, про последнее.

01:55:54.540 --> 01:55:56.580
А как набрать достаточно контекста?

01:55:56.580 --> 01:55:59.160
Это вот к вам вопрос, и это совершенно...

01:55:59.160 --> 01:56:01.460
То есть здесь мы кодируем этот раг, да?

01:56:02.340 --> 01:56:06.080
И здесь нам приходится как-то вот понимать,

01:56:06.360 --> 01:56:08.580
какие-то есть релевантные документы,

01:56:09.220 --> 01:56:10.880
а какие-то уже не надо откладывать.

01:56:11.300 --> 01:56:12.980
Здесь уже сложный вопрос.

01:56:13.100 --> 01:56:15.060
Ну, как бы кросс-энкодер это может сказать,

01:56:16.060 --> 01:56:19.560
что из предварительной выборки вот здесь уже идут

01:56:19.560 --> 01:56:21.580
нерелевантные документы, невключаемых вопросов.

01:56:21.580 --> 01:56:26.580
А вопрос про окончание генерации, это он прям очень насущный.

01:56:32.760 --> 01:56:36.380
Потому что, ну, часто бывает, что модель запутывается

01:56:36.380 --> 01:56:38.340
и начинает болтать бесконечно.

01:56:39.560 --> 01:56:40.720
Да, вот здесь вот осталось.

01:56:40.960 --> 01:56:41.920
А, finish, reason, stop.

01:56:42.680 --> 01:56:45.280
Ее тренировали останавливаться вовремя.

01:56:46.580 --> 01:56:49.220
И там отдельно параметры можно передать.

01:56:49.220 --> 01:56:52.420
Здесь, ну, мы часто передаем останавливаться

01:56:52.420 --> 01:56:57.080
после определенного длины сгенерированной.

01:56:57.680 --> 01:56:59.620
У нее есть специальные stop-токены.

01:56:59.820 --> 01:57:02.020
Ее вот в этих вот диалогах, когда тренировали,

01:57:02.100 --> 01:57:03.760
ей явно передавали stop-токен.

01:57:05.700 --> 01:57:08.320
Ну, и иногда она там теряет его.

01:57:09.700 --> 01:57:11.420
Вопрос очень насущный.

01:57:12.600 --> 01:57:13.860
В общем, если не предпринимать,

01:57:14.180 --> 01:57:15.500
если бы меры не предпринимались,

01:57:15.680 --> 01:57:17.180
оно бы генерировало бесконечно.

01:57:17.180 --> 01:57:21.160
А stop-токен – это что?

01:57:23.520 --> 01:57:26.500
Это типа какое-то насыщение какое-то, да,

01:57:26.560 --> 01:57:27.560
которое должно произойти?

01:57:28.600 --> 01:57:31.360
Это числовая характеристика какая-то или что это?

01:57:32.560 --> 01:57:34.520
Ну, это дрессировка, что в конце фразы

01:57:34.520 --> 01:57:35.840
обычно ставится точка.

01:57:36.500 --> 01:57:36.700
Вот.

01:57:36.780 --> 01:57:38.480
И когда ты эту точку видишь,

01:57:38.540 --> 01:57:40.180
остановись и дальше прекрати.

01:57:41.100 --> 01:57:42.660
Ну, то есть это окончание фразы,

01:57:42.760 --> 01:57:44.180
которое точно так же можно предсказать

01:57:44.180 --> 01:57:51.420
весом, как следующий next most possible token.

01:57:58.300 --> 01:58:00.220
Возвращаясь к первому вопросу, Андрей,

01:58:00.340 --> 01:58:02.640
там есть ряд техник по подготовке

01:58:02.640 --> 01:58:05.540
как раз документов для RAC, да,

01:58:05.660 --> 01:58:07.680
то есть есть ряд вещей, да,

01:58:07.740 --> 01:58:08.620
там какие файлы,

01:58:08.720 --> 01:58:10.180
какого формата она лучше понимает.

01:58:10.180 --> 01:58:12.880
И техники связаны с тем,

01:58:13.020 --> 01:58:14.960
что предварительно можно документы,

01:58:15.060 --> 01:58:18.580
опять-таки, через языковую модель прогнать,

01:58:19.240 --> 01:58:20.840
соответствующим образом обработать,

01:58:20.880 --> 01:58:25.340
а дальше уже загружать твое приложение

01:58:25.340 --> 01:58:31.140
в контент более компактной языковой модели

01:58:31.140 --> 01:58:34.220
для того, чтобы она по этим документам отвечала.

01:58:34.580 --> 01:58:37.060
То есть тут все как и у человека, да,

01:58:37.060 --> 01:58:40.200
то есть если ты в человека вгрузишь кучу буш,

01:58:40.340 --> 01:58:42.180
то, естественно, он запутается, да.

01:58:43.040 --> 01:58:44.440
Также языковые модели,

01:58:44.560 --> 01:58:48.860
то есть если ты дашь им достаточно структурированный документ,

01:58:49.180 --> 01:58:51.760
вот, и дальше будешь задавать вопросы

01:58:51.760 --> 01:58:52.780
по этому документу,

01:58:52.840 --> 01:58:55.480
то она ответит так же структурированно.

01:58:55.560 --> 01:58:57.820
Если ты дашь тысячи страниц,

01:58:58.080 --> 01:58:59.900
в которых сам теряешься,

01:59:00.060 --> 01:59:03.340
она тоже может потеряться примерно так.

01:59:03.340 --> 01:59:05.340
Понял, спасибо.

01:59:05.520 --> 01:59:08.520
Да, наиболее хороший формат – это Markdown,

01:59:08.820 --> 01:59:12.480
который вроде как все очень хорошо понимают,

01:59:12.560 --> 01:59:16.280
но здесь меня, может быть, Миша поправит дополнительно.

01:59:17.160 --> 01:59:22.560
Ну, это вроде последний такой, да, направление.

01:59:22.700 --> 01:59:24.260
Ну, и вот в пример про шахматы,

01:59:24.380 --> 01:59:25.760
если вы слайды посмотрите,

01:59:25.860 --> 01:59:28.200
вы увидите, что оно отвечает JSON,

01:59:28.500 --> 01:59:30.860
но JSON, он включен в Markdown-разметку.

01:59:30.860 --> 01:59:35.900
Да, их действительно тренируют Markdown, да.

01:59:36.900 --> 01:59:39.320
А про токены, смотрите, я вот вам говорил,

01:59:39.460 --> 01:59:41.780
что нам надо отправлять вот в структурном виде,

01:59:41.840 --> 01:59:42.920
да, в виде сообщений.

01:59:44.140 --> 01:59:46.760
И тут секрет в том, что внутри-то движок,

01:59:46.840 --> 01:59:48.920
который генерирует токены один за одним.

01:59:49.320 --> 01:59:50.900
И когда ее тренировали,

01:59:51.000 --> 01:59:53.580
вот делался FineTune на чат и Instruct,

01:59:54.680 --> 01:59:57.160
эти наши структурные, наш JSON,

01:59:57.380 --> 01:59:59.760
он размечался вот этими специальными токенами.

01:59:59.760 --> 02:00:02.860
Вот Inst, Inst — это вот системная инструкция,

02:00:03.240 --> 02:00:05.540
вот системный промпт проставлялся.

02:00:06.680 --> 02:00:08.700
А вот в этом конфиге,

02:00:08.720 --> 02:00:10.080
который на HagenFace лежит,

02:00:10.160 --> 02:00:11.060
SpecialTokenMap,

02:00:11.220 --> 02:00:12.880
вот эти вот токены,

02:00:12.940 --> 02:00:14.460
они спасаются от токенизации,

02:00:14.620 --> 02:00:18.060
чтобы их токенизатор не рвал на слоги.

02:00:18.760 --> 02:00:21.400
А те датасеты, которыми модель FineTune,

02:00:21.400 --> 02:00:27.120
вот там структурный диалог был переделан в текст

02:00:27.120 --> 02:00:29.640
с использованием вот этих вот токенов.

02:00:30.920 --> 02:00:33.480
И останавливаться она почему научилась?

02:00:33.640 --> 02:00:35.180
Потому что есть тут специальный,

02:00:35.180 --> 02:00:36.540
вот был специальный токен,

02:00:37.820 --> 02:00:38.780
SlashS,

02:00:39.780 --> 02:00:41.020
SlashS, видите, вот он говорит,

02:00:41.100 --> 02:00:42.680
что его не надо разрывать.

02:00:44.080 --> 02:00:45.940
А отдельно было сказано,

02:00:46.080 --> 02:00:47.360
что это EndOfStream токен.

02:00:47.360 --> 02:00:49.940
Вот, и на тех примерах диалогов,

02:00:49.980 --> 02:00:51.340
на которых она FineTune-илась,

02:00:51.440 --> 02:00:54.780
они оканчивались все на отчеркивание S.

02:00:55.500 --> 02:00:58.520
И теперь, когда она генерирует,

02:00:58.720 --> 02:01:01.480
и почему-то она сгенерировала SlashS,

02:01:01.480 --> 02:01:04.100
она прекращает генерацию.

02:01:06.500 --> 02:01:08.060
Так это внутри работало.

02:01:08.220 --> 02:01:10.700
Еще про обучение хотелось сказать.

02:01:10.720 --> 02:01:11.600
Самое загадочное,

02:01:11.700 --> 02:01:13.560
почему-то она сгенерировала SlashS.

02:01:15.080 --> 02:01:16.800
Потому что она прочитала

02:01:16.800 --> 02:01:18.040
тысячи диалогов,

02:01:18.800 --> 02:01:21.640
которые в ответ на голубое небо,

02:01:21.860 --> 02:01:23.640
она говорила, говорила, говорила,

02:01:24.160 --> 02:01:25.860
а потом они заканчивались

02:01:25.860 --> 02:01:27.180
на подчеркивание S.

02:01:30.420 --> 02:01:31.640
То есть тут можно представить,

02:01:31.800 --> 02:01:34.040
что у нас же дискретный ввод, да?

02:01:34.220 --> 02:01:35.100
Вот представьте,

02:01:35.360 --> 02:01:36.280
что вы такой оригинальный,

02:01:36.360 --> 02:01:36.980
что туда ввели,

02:01:38.100 --> 02:01:39.520
этот уже диалог происходил

02:01:39.520 --> 02:01:40.500
в тестовой выборке.

02:01:40.820 --> 02:01:41.420
Ну, в принципе,

02:01:41.580 --> 02:01:42.540
оно же все счетно.

02:01:42.960 --> 02:01:43.440
Похожий.

02:01:43.900 --> 02:01:44.500
Ну, то есть, по сути,

02:01:44.500 --> 02:01:46.600
у нее есть какая-то нейронка отдельная,

02:01:46.680 --> 02:01:47.260
под нейронка,

02:01:47.400 --> 02:01:48.100
которая, по сути,

02:01:48.960 --> 02:01:52.060
является нашим интуитивным пониманием,

02:01:52.260 --> 02:01:54.700
что мы слишком много болтаем.

02:01:54.900 --> 02:01:55.640
Вот я в какой-то момент

02:01:55.640 --> 02:01:56.460
сейчас остановлюсь,

02:01:56.540 --> 02:01:57.780
потому что до хера говорю, да?

02:01:59.120 --> 02:02:00.440
Она получает контекст,

02:02:00.520 --> 02:02:00.800
она видит,

02:02:00.880 --> 02:02:01.580
что она, наверное,

02:02:01.660 --> 02:02:02.360
дофига сказала,

02:02:02.560 --> 02:02:03.560
и вот надо остановиться.

02:02:03.780 --> 02:02:04.140
Парабах.

02:02:04.140 --> 02:02:06.820
Нет, у меня нет никакой модели.

02:02:06.820 --> 02:02:09.280
Мы мысль закончена на самом деле.

02:02:09.460 --> 02:02:10.740
То есть там подмодели-то нет,

02:02:10.860 --> 02:02:12.300
просто мысль заканчивается,

02:02:12.460 --> 02:02:12.620
все.

02:02:13.380 --> 02:02:13.700
Дальше.

02:02:13.780 --> 02:02:14.240
Но мысль,

02:02:14.360 --> 02:02:15.480
но она же несколько мыслей

02:02:15.480 --> 02:02:15.980
может сказать.

02:02:16.340 --> 02:02:17.620
Но она же несколько фраз

02:02:17.620 --> 02:02:18.160
может сказать,

02:02:18.260 --> 02:02:19.360
она же несколько предложений

02:02:19.360 --> 02:02:20.360
с точкой может сказать,

02:02:20.440 --> 02:02:20.640
правильно?

02:02:21.860 --> 02:02:23.220
В тестовых выборках

02:02:23.220 --> 02:02:24.580
она отвечала там

02:02:24.580 --> 02:02:25.300
тремя фразами.

02:02:26.480 --> 02:02:27.260
Я вот это,

02:02:27.380 --> 02:02:28.420
я не представляю там

02:02:28.420 --> 02:02:30.080
внутренние нейронки у нейронки,

02:02:30.160 --> 02:02:31.000
я представляю так,

02:02:31.100 --> 02:02:31.940
что все это уже было.

02:02:32.260 --> 02:02:33.180
Она все эти диалоги

02:02:33.180 --> 02:02:34.100
уже проговорила.

02:02:35.120 --> 02:02:35.940
Мне кажется, так.

02:02:37.180 --> 02:02:38.460
Я так это себе моделирую.

02:02:39.260 --> 02:02:40.500
Про мощность обучения,

02:02:40.600 --> 02:02:41.460
да, почему она такая умная?

02:02:41.560 --> 02:02:42.480
Потому что, ну,

02:02:42.560 --> 02:02:43.960
весь прорыв в этом ГПТ,

02:02:44.100 --> 02:02:44.920
он был в том,

02:02:45.000 --> 02:02:46.220
что они придумали,

02:02:46.380 --> 02:02:47.460
как их можно тренировать.

02:02:47.580 --> 02:02:48.240
Первый этап –

02:02:48.240 --> 02:02:48.900
это третрейн.

02:02:49.460 --> 02:02:50.420
Претрейн –

02:02:50.420 --> 02:02:51.760
это просто оно все языки

02:02:51.760 --> 02:02:52.300
из интернета,

02:02:52.460 --> 02:02:52.640
которые,

02:02:52.780 --> 02:02:54.100
все тексты,

02:02:54.100 --> 02:02:55.220
оно, значит, съела,

02:02:55.600 --> 02:02:56.440
и оно знает,

02:02:56.580 --> 02:02:57.640
какое слово за каким идет.

02:02:58.560 --> 02:03:00.500
А потом она тренируется

02:03:00.500 --> 02:03:02.600
на многих именно

02:03:02.600 --> 02:03:03.760
тестовых диалогах,

02:03:03.760 --> 02:03:04.460
как бы говоря,

02:03:04.540 --> 02:03:05.700
с ней уже поговорили там

02:03:05.700 --> 02:03:06.120
про небо,

02:03:06.200 --> 02:03:07.500
обсудили какие-то,

02:03:07.560 --> 02:03:08.740
вот много диалогов таких

02:03:08.740 --> 02:03:09.860
уже случилось с ней.

02:03:09.860 --> 02:03:12.020
А потом еще

02:03:12.020 --> 02:03:14.440
ей задавали другие диалоги,

02:03:14.660 --> 02:03:15.700
и ответы ее

02:03:15.700 --> 02:03:18.280
ранжировали.

02:03:19.640 --> 02:03:23.380
И вот получается,

02:03:23.640 --> 02:03:24.140
что она

02:03:24.140 --> 02:03:27.180
ответы на диалоги

02:03:27.180 --> 02:03:28.540
пыталась давать,

02:03:29.280 --> 02:03:29.600
помня,

02:03:29.680 --> 02:03:30.320
те диалоги,

02:03:30.480 --> 02:03:31.000
которые были,

02:03:31.300 --> 02:03:32.340
и тот огромный объем

02:03:32.340 --> 02:03:33.300
интернетных текстов,

02:03:33.420 --> 02:03:34.820
которые она съела на претрейне.

02:03:34.820 --> 02:03:36.840
оттуда она делала

02:03:36.840 --> 02:03:37.540
гипотезы,

02:03:37.620 --> 02:03:39.320
как могут продолжаться

02:03:39.320 --> 02:03:39.860
диалоги,

02:03:39.900 --> 02:03:40.660
которых она еще

02:03:40.660 --> 02:03:42.420
не видела.

02:03:43.240 --> 02:03:44.020
А эксперты

02:03:44.020 --> 02:03:44.540
с помощью

02:03:44.540 --> 02:03:45.240
Relichef

02:03:45.240 --> 02:03:46.800
отдельного

02:03:46.800 --> 02:03:47.800
процедуры обучения

02:03:47.800 --> 02:03:48.740
подтвердили ее

02:03:48.740 --> 02:03:49.240
догадки.

02:03:51.060 --> 02:03:51.460
То есть там

02:03:51.460 --> 02:03:52.240
минимум есть

02:03:52.240 --> 02:03:53.260
три различных

02:03:53.260 --> 02:03:53.800
этапа

02:03:53.800 --> 02:03:55.040
тренировки,

02:03:55.040 --> 02:03:55.800
но

02:03:55.800 --> 02:03:58.080
дата-стантисты,

02:03:58.200 --> 02:03:58.380
наверное,

02:03:58.780 --> 02:04:01.000
более точно

02:04:01.000 --> 02:04:01.420
дадут

02:04:01.420 --> 02:04:02.420
ответ на этот вопрос.

02:04:03.060 --> 02:04:03.500
Но вот у меня

02:04:03.500 --> 02:04:04.200
такая модель

02:04:04.200 --> 02:04:05.040
из трех этапов.

02:04:12.840 --> 02:04:13.440
Спасибо,

02:04:13.580 --> 02:04:14.160
очень интересный

02:04:14.160 --> 02:04:14.520
воркшоп.

02:04:14.700 --> 02:04:14.780
Да,

02:04:15.000 --> 02:04:15.720
на Deep Learning

02:04:15.720 --> 02:04:16.920
есть курс,

02:04:17.360 --> 02:04:17.560
где

02:04:17.560 --> 02:04:18.340
Эндрю Ин

02:04:18.340 --> 02:04:19.400
рассказывает

02:04:19.400 --> 02:04:20.400
и про варианты

02:04:20.400 --> 02:04:21.000
обучения,

02:04:21.260 --> 02:04:21.680
про то,

02:04:21.860 --> 02:04:22.740
какие языковые

02:04:22.740 --> 02:04:23.560
модели бывают.

02:04:23.700 --> 02:04:24.120
Я сейчас пытаюсь

02:04:24.120 --> 02:04:25.240
что-то сходу найти.

02:04:26.620 --> 02:04:27.340
Можете просто

02:04:27.340 --> 02:04:27.760
посмотреть,

02:04:27.920 --> 02:04:28.960
там порядка

02:04:28.960 --> 02:04:30.720
6-7-8

02:04:30.720 --> 02:04:31.600
видео обычно

02:04:31.600 --> 02:04:33.360
по 10 минут.

02:04:34.480 --> 02:04:34.900
И там

02:04:34.900 --> 02:04:36.220
и варианты

02:04:36.220 --> 02:04:36.700
обучения

02:04:36.700 --> 02:04:37.580
рассказываются,

02:04:37.740 --> 02:04:38.580
и как устроено,

02:04:38.920 --> 02:04:40.340
и как работает

02:04:40.340 --> 02:04:41.700
на верхнем уровне.

02:04:41.740 --> 02:04:42.000
При этом

02:04:42.000 --> 02:04:42.520
в деталях

02:04:42.520 --> 02:04:43.400
никто не понимает,

02:04:43.540 --> 02:04:44.100
потому что

02:04:44.100 --> 02:04:46.240
она сама

02:04:46.240 --> 02:04:47.180
это делает.

02:04:48.080 --> 02:04:48.680
Вот ей что-то

02:04:48.680 --> 02:04:49.240
скормили,

02:04:49.480 --> 02:04:49.920
а дальше

02:04:49.920 --> 02:04:50.520
она сама

02:04:50.520 --> 02:04:51.880
каким-то образом

02:04:51.880 --> 02:04:52.460
на

02:04:52.460 --> 02:04:53.200
код А

02:04:53.200 --> 02:04:53.820
генерирует

02:04:53.820 --> 02:04:54.120
выход

02:04:54.120 --> 02:04:54.420
Б.

02:04:55.760 --> 02:04:56.560
И она

02:04:56.560 --> 02:04:58.000
принимает

02:04:58.000 --> 02:04:58.400
решение,

02:04:58.520 --> 02:04:59.100
почему Б

02:04:59.100 --> 02:04:59.440
и когда

02:04:59.440 --> 02:05:00.200
остановиться.

02:05:02.120 --> 02:05:02.860
Если кто-то

02:05:02.860 --> 02:05:03.220
из ребят

02:05:03.220 --> 02:05:03.620
помнит

02:05:03.620 --> 02:05:04.160
про этот

02:05:04.160 --> 02:05:04.680
тренинг,

02:05:04.760 --> 02:05:05.920
Леша Киселев

02:05:05.920 --> 02:05:06.280
либо

02:05:06.280 --> 02:05:08.520
Миша сам.

02:05:08.520 --> 02:05:09.340
Походу

02:05:09.340 --> 02:05:09.760
не найду

02:05:09.760 --> 02:05:10.100
тоже.

02:05:10.360 --> 02:05:11.040
У Гугла

02:05:11.040 --> 02:05:11.300
есть

02:05:11.300 --> 02:05:11.820
замечательный

02:05:11.820 --> 02:05:12.460
визуализатор,

02:05:12.680 --> 02:05:12.960
который

02:05:12.960 --> 02:05:14.300
сквозь все

02:05:14.300 --> 02:05:14.840
слои

02:05:14.840 --> 02:05:15.560
трансформеров

02:05:15.560 --> 02:05:16.160
показывает

02:05:16.160 --> 02:05:16.800
активацию

02:05:16.800 --> 02:05:17.540
при отработке

02:05:17.540 --> 02:05:18.260
запроса.

02:05:18.960 --> 02:05:19.040
Ну,

02:05:19.660 --> 02:05:20.080
как бы

02:05:20.080 --> 02:05:21.360
для моделей

02:05:21.360 --> 02:05:22.460
такого размера

02:05:22.460 --> 02:05:22.800
штука

02:05:22.800 --> 02:05:23.800
совершенно бесполезная,

02:05:23.880 --> 02:05:24.440
хотя и

02:05:24.440 --> 02:05:25.000
красиво

02:05:25.000 --> 02:05:25.740
выглядящая.

02:05:26.200 --> 02:05:26.360
Ну,

02:05:26.480 --> 02:05:26.880
потому что

02:05:26.880 --> 02:05:27.580
активируются

02:05:27.580 --> 02:05:28.480
абсолютно разные.

02:05:28.640 --> 02:05:28.680
Ну,

02:05:28.760 --> 02:05:29.200
вот похоже

02:05:29.200 --> 02:05:30.100
на энцефалограмму,

02:05:30.140 --> 02:05:30.380
наверное,

02:05:30.960 --> 02:05:31.280
больше,

02:05:31.400 --> 02:05:33.260
когда слои...

02:05:33.260 --> 02:05:33.720
Нет,

02:05:34.000 --> 02:05:35.340
томограмму,

02:05:35.340 --> 02:05:35.900
когда мозг

02:05:35.900 --> 02:05:36.480
снимают

02:05:36.480 --> 02:05:37.000
там

02:05:37.000 --> 02:05:37.920
на шутку,

02:05:38.220 --> 02:05:39.440
на выстрел

02:05:39.440 --> 02:05:41.020
и на еще что-то.

02:05:41.140 --> 02:05:41.400
Примерно

02:05:41.400 --> 02:05:41.940
так же у нее

02:05:41.940 --> 02:05:42.500
вспыхивает

02:05:42.500 --> 02:05:43.100
по слоям

02:05:43.100 --> 02:05:43.780
активация.

02:05:51.480 --> 02:05:52.040
Так,

02:05:52.220 --> 02:05:52.360
ну,

02:05:52.560 --> 02:05:52.780
у меня

02:05:52.780 --> 02:05:53.580
все.

02:05:54.340 --> 02:05:54.820
Переходим

02:05:54.820 --> 02:05:55.180
в этот

02:05:55.180 --> 02:05:57.100
онлайн

02:05:57.100 --> 02:05:57.620
формат.

02:05:58.620 --> 02:06:00.200
Я с удовольствием

02:06:00.200 --> 02:06:00.820
отвечу на вопрос.

02:06:00.940 --> 02:06:01.600
Еще какой-то вопрос?

02:06:02.320 --> 02:06:02.840
Да,

02:06:02.920 --> 02:06:03.560
я предлагаю

02:06:03.560 --> 02:06:04.380
уже запись

02:06:04.380 --> 02:06:05.040
закончить.

02:06:05.340 --> 02:06:05.940
Да.

02:06:06.100 --> 02:06:06.660
Со мной

02:06:06.660 --> 02:06:07.220
прошли,

02:06:07.400 --> 02:06:08.160
а все вопросы

02:06:08.160 --> 02:06:10.040
ответили.

